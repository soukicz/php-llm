{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PHP LLM - Agentic AI Framework for PHP","text":"<p>Build powerful AI agents that can use tools, self-correct, and take autonomous actions. A unified PHP framework for Large Language Models with support for Anthropic Claude, OpenAI GPT, Google Gemini, and more.</p> <p>What is Agentic AI? Agents that can call functions, validate outputs, iterate on responses, and make decisions autonomously - not just generate text.</p> <pre><code>composer require soukicz/llm\n</code></pre>"},{"location":"#why-php-llm","title":"Why PHP LLM?","text":"<ul> <li>\ud83e\udd16 Build AI Agents - Create autonomous agents with tools, feedback loops, and state management</li> <li>\ud83d\udd04 Unified API - One interface for Anthropic, OpenAI, Gemini, and more</li> <li>\ud83d\udee0\ufe0f Function Calling - Empower agents to interact with external systems and APIs</li> <li>\ud83d\udcdd Built-in Tools - TextEditorTool for file manipulation, embeddings API, and more</li> <li>\u2705 Self-Correcting - Validate and refine outputs with feedback loops</li> <li>\ud83d\udcf8 Multimodal - Process images and PDFs alongside text (with caching support)</li> <li>\ud83e\udde0 Reasoning Models - Advanced thinking with o3 and o4-mini reasoning models</li> <li>\u26a1 Async &amp; Caching - Fast, cost-effective operations with prompt caching</li> <li>\ud83d\udcbe State Persistence - Save and resume conversations with thread IDs</li> <li>\ud83d\udcca Monitoring - Built-in logging, cost tracking, and debugging interfaces</li> </ul>"},{"location":"#key-concepts","title":"Key Concepts","text":"<p>Before you start, understanding these core concepts will help you use the library effectively:</p>"},{"location":"#async-by-default","title":"Async by Default","text":"<p>All LLM clients in this library are asynchronous by default using Guzzle Promises. The <code>run()</code> method is a convenience wrapper that calls <code>runAsync()-&gt;wait()</code> internally. For production applications handling multiple requests, use the async methods directly for better performance.</p>"},{"location":"#two-types-of-clients","title":"Two Types of Clients","text":"<ul> <li> <p>LLM Clients (<code>AnthropicClient</code>, <code>OpenAIClient</code>, etc.) - Low-level API clients that send a single request and return a single response. Use these when you need direct control over individual API calls.</p> </li> <li> <p>Chain Client (<code>LLMChainClient</code>) - High-level orchestrator that handles multi-turn conversations, automatic tool calling, feedback loops, and retries. Use this for building agents that need to iterate or use tools.</p> </li> </ul>"},{"location":"#model-versions","title":"Model Versions","text":"<p>Anthropic and OpenAI models require explicit version constants: <pre><code>&lt;?php\nnew AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929)\nnew GPTo3(GPTo3::VERSION_2025_04_16)\n</code></pre> Google Gemini models do NOT require versions - just instantiate them directly.</p>"},{"location":"#conversations-state","title":"Conversations &amp; State","text":"<p><code>LLMConversation</code> manages the message history and can be serialized/deserialized for persistence. Each conversation has an optional <code>threadId</code> (UUID) for tracking across sessions.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>&lt;?php\nrequire_once __DIR__ . '/vendor/autoload.php';\n\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\nuse Soukicz\\Llm\\Client\\LLMChainClient;\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\n\n// Optional: Enable prompt caching to reduce costs\n$cache = new FileCache(sys_get_temp_dir());\n\n// Create the API client (low-level, sends single requests)\n$client = new AnthropicClient('sk-xxxxx', $cache);\n\n// Create the chain client (high-level, handles tool calls and feedback loops)\n$chainClient = new LLMChainClient();\n\n// Run a request (this is synchronous - use runAsync() for better performance)\n$response = $chainClient-&gt;run(\n    client: $client,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString('What is PHP?')\n        ]),\n    )\n);\n\n// Get the assistant's response text\necho $response-&gt;getLastText();\n</code></pre>"},{"location":"#async-usage","title":"Async Usage","text":"<pre><code>&lt;?php\n// For better performance, use async operations\n$promise = $chainClient-&gt;runAsync($client, $request);\n\n$promise-&gt;then(\n    function (LLMResponse $response) {\n        echo $response-&gt;getLastText();\n    },\n    function (Exception $error) {\n        echo \"Error: \" . $error-&gt;getMessage();\n    }\n);\n</code></pre>"},{"location":"#provider-specific-setup","title":"Provider-Specific Setup","text":"<pre><code>&lt;?php\n// Anthropic Claude\n$client = new AnthropicClient(\n    apiKey: 'sk-ant-xxxxx',\n    cache: $cache,\n    customHttpMiddleware: null,\n    betaFeatures: [] // e.g., ['text-editor-20250116'] for TextEditorTool\n);\n\n// OpenAI (organization parameter is required)\n$client = new OpenAIClient(\n    apiKey: 'sk-xxxxx',\n    apiOrganization: 'org-xxxxx', // Required parameter\n    cache: $cache\n);\n\n// Google Gemini\n$client = new GeminiClient(\n    apiKey: 'your-key',\n    cache: $cache\n);\n</code></pre> <p>\u2192 More Examples</p>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#function-calling-tools","title":"\ud83d\udee0\ufe0f Function Calling (Tools)","text":"<p>Enable AI agents to call external functions and APIs:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Tool\\CallbackToolDefinition;\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\n\n$weatherTool = new CallbackToolDefinition(\n    name: 'get_weather',\n    description: 'Get current weather for a location',\n    inputSchema: ['type' =&gt; 'object', 'properties' =&gt; ['city' =&gt; ['type' =&gt; 'string']]],\n    handler: fn($input) =&gt; LLMMessageContents::fromArrayData([\n        'temperature' =&gt; 22,\n        'condition' =&gt; 'sunny'\n    ])\n);\n\n$response = $chainClient-&gt;run($client, new LLMRequest(\n    model: $model,\n    conversation: $conversation,\n    tools: [$weatherTool],\n));\n</code></pre> <p>Note: Tool handlers must return <code>LLMMessageContents</code> or a Promise. See Tools Documentation for complete examples.</p> <p>\u2192 Tools Documentation</p>"},{"location":"#feedback-loops","title":"\u2705 Feedback Loops","text":"<p>Build self-correcting agents that validate and improve their outputs:</p> <pre><code>&lt;?php\n$response = $chainClient-&gt;run(\n    client: $client,\n    request: $request,\n    feedbackCallback: function ($response) {\n        if (!isValid($response-&gt;getLastText())) {\n            return LLMMessage::createFromUserString('Please try again with valid JSON');\n        }\n        return null; // Valid, stop iteration\n    }\n);\n</code></pre> <p>\u2192 Feedback Loops Documentation</p>"},{"location":"#multimodal-support","title":"\ud83d\udcf8 Multimodal Support","text":"<p>Process images and PDFs alongside text:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\nuse Soukicz\\Llm\\Message\\LLMMessageImage;\nuse Soukicz\\Llm\\Message\\LLMMessagePdf;\nuse Soukicz\\Llm\\Message\\LLMMessageText;\n\n// Images\n$imageData = base64_encode(file_get_contents('/path/to/image.jpg'));\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('What is in this image?'),\n    new LLMMessageImage('base64', 'image/jpeg', $imageData, cached: true) // Enable prompt caching\n]));\n\n// PDFs\n$pdfData = base64_encode(file_get_contents('/path/to/document.pdf'));\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('Summarize this document'),\n    new LLMMessagePdf('base64', $pdfData, cached: true) // Optimize with caching\n]));\n</code></pre> <p>Tip: Use the <code>cached: true</code> parameter on large images/PDFs to enable prompt caching and reduce costs.</p> <p>\u2192 Multimodal Documentation</p>"},{"location":"#reasoning-models","title":"\ud83e\udde0 Reasoning Models","text":"<p>Use advanced reasoning for complex problems:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Config\\ReasoningEffort;\nuse Soukicz\\Llm\\Config\\ReasoningBudget;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPT5;\n\n// Control reasoning with effort level (for supported models)\n$request = new LLMRequest(\n    model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n    conversation: $conversation,\n    reasoningConfig: ReasoningEffort::HIGH // LOW, MEDIUM, or HIGH\n);\n\n// Or use token-based budget control (for supported models)\n$request = new LLMRequest(\n    model: new GPT5(GPT5::VERSION_2025_08_07),\n    conversation: $conversation,\n    reasoningConfig: new ReasoningBudget(10000) // Max reasoning tokens\n);\n</code></pre> <p>\u2192 Reasoning Models Documentation</p>"},{"location":"#advanced-features","title":"Advanced Features","text":""},{"location":"#texteditortool-built-in-file-manipulation","title":"\ud83d\udcdd TextEditorTool - Built-in File Manipulation","text":"<p>Empower agents to read, write, and manage files with the built-in TextEditorTool:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Tool\\TextEditorTool;\nuse Soukicz\\Llm\\Tool\\TextEditorStorageFilesystem;\n\n// Create filesystem storage with sandboxing\n$storage = new TextEditorStorageFilesystem('/safe/workspace/path');\n$textEditorTool = new TextEditorTool($storage);\n\n// Enable for Anthropic Claude with beta features\n$client = new AnthropicClient(\n    apiKey: 'sk-ant-xxxxx',\n    cache: $cache,\n    betaFeatures: ['text-editor-20250116'] // Required for TextEditorTool\n);\n\n$response = $chainClient-&gt;run($client, new LLMRequest(\n    model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n    conversation: new LLMConversation([\n        LLMMessage::createFromUserString('Create a PHP file with a hello world function')\n    ]),\n    tools: [$textEditorTool]\n));\n</code></pre> <p>\u2192 Tools Documentation for complete TextEditorTool examples</p>"},{"location":"#embeddings-api","title":"\ud83d\udd22 Embeddings API","text":"<p>Generate embeddings for semantic search, clustering, and RAG applications:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAIClient;\n\n$client = new OpenAIClient('sk-xxxxx', 'your-org-id');\n\n$embeddings = $client-&gt;getBatchEmbeddings(\n    texts: ['Hello world', 'PHP is great', 'AI embeddings'],\n    model: 'text-embedding-3-small',\n    dimensions: 512\n);\n\n// Returns array of float arrays (embeddings)\nforeach ($embeddings as $i =&gt; $embedding) {\n    echo \"Text {$i} embedding dimensions: \" . count($embedding) . \"\\n\";\n}\n</code></pre>"},{"location":"#monitoring-debugging","title":"\ud83d\udcca Monitoring &amp; Debugging","text":"<p>Built-in interfaces for logging and monitoring:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Log\\LLMLogger;\n\n// Implement custom logger\nclass MyLogger implements LLMLogger {\n    public function log(LLMRequest $request, LLMResponse $response): void {\n        // Log requests, responses, costs, tokens, etc.\n        $cost = ($response-&gt;getInputPriceUsd() ?? 0) + ($response-&gt;getOutputPriceUsd() ?? 0);\n        echo \"Cost: $\" . $cost . \"\\n\";\n        echo \"Tokens: {$response-&gt;getInputTokens()} in, {$response-&gt;getOutputTokens()} out\\n\";\n    }\n}\n\n// Attach to chain client\n$chainClient = new LLMChainClient(logger: new MyLogger());\n</code></pre> <p>\u2192 Logging &amp; Debugging Documentation</p>"},{"location":"#advanced-request-configuration","title":"\u2699\ufe0f Advanced Request Configuration","text":"<p>Fine-tune your requests with additional parameters:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\LLMRequest;\n\n$request = new LLMRequest(\n    model: $model,\n    conversation: $conversation,\n    tools: $tools,\n\n    // Custom stop sequences to halt generation\n    stopSequences: ['END', '---'],\n\n    // Reasoning configuration (for o3/o4-mini models)\n    reasoningConfig: ReasoningEffort::HIGH,\n    // OR\n    reasoningConfig: new ReasoningBudget(10000),\n);\n\n// Access cost and token information\n$response = $chainClient-&gt;run($client, $request);\n$cost = ($response-&gt;getInputPriceUsd() ?? 0) + ($response-&gt;getOutputPriceUsd() ?? 0);\necho \"Cost: $\" . $cost . \"\\n\";\necho \"Input tokens: \" . $response-&gt;getInputTokens() . \"\\n\";\necho \"Output tokens: \" . $response-&gt;getOutputTokens() . \"\\n\";\necho \"Stop reason: \" . $response-&gt;getStopReason()-&gt;value . \"\\n\"; // END_TURN, TOOL_USE, MAX_TOKENS, STOP_SEQUENCE\n</code></pre>"},{"location":"#supported-providers","title":"Supported Providers","text":"<ul> <li>Anthropic (Claude) - Claude 3.5, 3.7, 4.0, 4.1, and 4.5 series models</li> <li>OpenAI (GPT) - GPT-4o, GPT-4.1, o3 and o4-mini (reasoning), and GPT-5 series models</li> <li>Google Gemini - Gemini 2.0 and 2.5 series models</li> <li>OpenAI-Compatible - OpenRouter, local servers (Ollama, llama-server), and more</li> <li>AWS Bedrock - Via separate package (<code>soukicz/llm-aws-bedrock</code>)</li> </ul> <p>\u2192 Provider Comparison</p>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start Examples - Get up and running in minutes</li> <li>Configuration Guide - Configure clients and requests</li> <li>Provider Overview - Choose the right provider</li> <li>Best Practices - Production-ready patterns</li> </ul>"},{"location":"#core-features_1","title":"Core Features","text":"<ul> <li>Tools &amp; Function Calling - External tools, TextEditorTool, custom functions</li> <li>Feedback Loops - Self-correcting agents and validation</li> <li>Multimodal Support - Images, PDFs, and caching</li> <li>Reasoning Models - o3/o4-mini with effort and budget control</li> </ul>"},{"location":"#advanced-features_1","title":"Advanced Features","text":"<ul> <li>Caching - Prompt caching and cost reduction</li> <li>Batch Processing - High-volume async operations</li> <li>State Management - Persistence and thread IDs</li> <li>Logging &amp; Debugging - Monitor and debug</li> </ul>"},{"location":"#common-use-cases","title":"Common Use Cases","text":""},{"location":"#ai-agent-with-tools","title":"AI Agent with Tools","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Tool\\CallbackToolDefinition;\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\n\n// Create custom tools for the agent\n$calculatorTool = new CallbackToolDefinition(\n    name: 'calculate',\n    description: 'Perform mathematical calculations',\n    inputSchema: [\n        'type' =&gt; 'object',\n        'properties' =&gt; [\n            'expression' =&gt; ['type' =&gt; 'string', 'description' =&gt; 'Math expression to evaluate']\n        ]\n    ],\n    handler: fn($input) =&gt; LLMMessageContents::fromArrayData([\n        'result' =&gt; eval('return ' . $input['expression'] . ';')\n    ])\n);\n\n$searchTool = new CallbackToolDefinition(\n    name: 'search_database',\n    description: 'Search the product database',\n    inputSchema: [\n        'type' =&gt; 'object',\n        'properties' =&gt; [\n            'query' =&gt; ['type' =&gt; 'string']\n        ]\n    ],\n    handler: function($input) use ($pdo) {\n        $stmt = $pdo-&gt;prepare('SELECT * FROM products WHERE name LIKE ?');\n        $stmt-&gt;execute(['%' . $input['query'] . '%']);\n        return LLMMessageContents::fromArrayData($stmt-&gt;fetchAll());\n    }\n);\n\n// Agent will automatically use tools as needed\n$response = $chainClient-&gt;run($client, new LLMRequest(\n    model: $model,\n    conversation: new LLMConversation([\n        LLMMessage::createFromUserString('Find products with \"laptop\" and calculate 15% discount on $999')\n    ]),\n    tools: [$searchTool, $calculatorTool],\n));\n</code></pre>"},{"location":"#self-correcting-json-parser","title":"Self-Correcting JSON Parser","text":"<pre><code>&lt;?php\n// Agent that validates and corrects its own output\n$response = $chainClient-&gt;run(\n    client: $client,\n    request: new LLMRequest(\n        model: $model,\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString('Extract user data as JSON: John Doe, age 30, email john@example.com')\n        ])\n    ),\n    feedbackCallback: function ($response) {\n        $text = $response-&gt;getLastText();\n        json_decode($text);\n\n        if (json_last_error() !== JSON_ERROR_NONE) {\n            return LLMMessage::createFromUserString(\n                'Invalid JSON: ' . json_last_error_msg() . '. Please fix the syntax.'\n            );\n        }\n\n        return null; // Valid JSON, stop iteration\n    },\n    maxIterations: 3 // Limit retry attempts\n);\n</code></pre>"},{"location":"#multimodal-document-analysis","title":"Multimodal Document Analysis","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Message\\{LLMMessageContents, LLMMessageText, LLMMessageImage, LLMMessagePdf};\n\n// Agent that analyzes multiple document types\n$chartData = base64_encode(file_get_contents('/sales-chart.png'));\n$reportData = base64_encode(file_get_contents('/quarterly-report.pdf'));\n\n$response = $chainClient-&gt;run($client, new LLMRequest(\n    model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n    conversation: new LLMConversation([\n        LLMMessage::createFromUser(new LLMMessageContents([\n            new LLMMessageText('Analyze these documents and summarize the key insights'),\n            new LLMMessageImage('base64', 'image/png', $chartData, cached: true),\n            new LLMMessagePdf('base64', $reportData, cached: true),\n        ]))\n    ])\n));\n\necho $response-&gt;getLastText();\n</code></pre>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"#whats-the-difference-between-agentic-and-regular-llm-usage","title":"What's the difference between \"agentic\" and regular LLM usage?","text":"<p>Agentic AI refers to LLMs that can autonomously take actions, use tools, and iterate on their responses. Instead of just generating text, agentic systems: - Call external functions and APIs (tool use) - Validate and self-correct their outputs (feedback loops) - Make decisions about which tools to use - Persist state across multiple interactions</p> <p>This library is designed specifically to make building such agents easy in PHP.</p>"},{"location":"#how-do-i-reduce-api-costs","title":"How do I reduce API costs?","text":"<ol> <li>Enable caching: Pass a <code>FileCache</code> instance to reduce repeated prompts</li> <li>Use prompt caching: Set <code>cached: true</code> on images/PDFs</li> <li>Choose appropriate models: Smaller models for simpler tasks</li> <li>Use stop sequences: Define custom stop sequences to prevent over-generation</li> </ol>"},{"location":"#can-i-use-this-with-local-models","title":"Can I use this with local models?","text":"<p>Yes! Use the <code>OpenAICompatibleClient</code> to connect to: - Ollama (local models) - llama-server - OpenRouter - Any service with OpenAI-compatible API</p>"},{"location":"#how-do-i-save-and-resume-conversations","title":"How do I save and resume conversations?","text":"<pre><code>&lt;?php\n// Save conversation\n$json = json_encode($conversation);\nfile_put_contents('conversation.json', $json);\n\n// Resume conversation\n$data = json_decode(file_get_contents('conversation.json'), true);\n$conversation = LLMConversation::fromJson($data);\n</code></pre>"},{"location":"#development","title":"Development","text":""},{"location":"#running-tests","title":"Running Tests","text":"<pre><code># Copy environment template\ncp .env.example .env\n\n# Add your API keys to .env\n# ANTHROPIC_API_KEY=sk-ant-xxxxx\n# OPENAI_API_KEY=sk-xxxxx\n# GEMINI_API_KEY=your-key\n\n# Run tests\nvendor/bin/phpunit\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>PHP 8.3 or higher</li> <li>Composer</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p>"},{"location":"#license","title":"License","text":"<p>This project is open-sourced software licensed under the BSD-3-Clause license.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>Documentation - Full documentation</li> <li>GitHub - Source code</li> <li>Packagist - Composer package</li> </ul> <p>Built for modern PHP \u2022 Requires PHP 8.3+ \u2022 BSD-3-Clause Licensed</p>"},{"location":"examples/","title":"Examples","text":"<p>Practical, copy-paste ready examples to help you get started with PHP LLM and build powerful AI applications.</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start - Get up and running in minutes with basic examples for simple synchronous requests, conversation management, and streaming responses.</li> </ul>"},{"location":"examples/#core-functionality","title":"Core Functionality","text":"<ul> <li> <p>Tools &amp; Function Calling - Enable AI agents to interact with external systems, databases, APIs, and custom code. Learn how to create tools, handle parallel execution, and build intelligent agents.</p> </li> <li> <p>Multimodal - Process images and PDFs with AI models. Perfect for document analysis, visual understanding, and data extraction tasks.</p> </li> <li> <p>State Management - Manage conversation history, implement persistent sessions, and build stateful AI applications.</p> </li> </ul>"},{"location":"examples/#production-patterns","title":"Production Patterns","text":"<ul> <li> <p>Logging &amp; Debugging - Monitor your AI applications with comprehensive logging, debugging tools, and performance tracking.</p> </li> <li> <p>Best Practices - Security, performance optimization, cost management, and architectural patterns for building production-ready AI applications.</p> </li> </ul>"},{"location":"examples/#what-youll-learn","title":"What You'll Learn","text":"<p>These examples cover: - Basic usage: Simple requests, conversations, streaming - Advanced features: Tools, multimodal, caching, reasoning models - Production patterns: Error handling, logging, retries, resilience - Best practices: Security, performance, cost optimization - Real-world scenarios: Practical code you can adapt to your needs</p> <p>All examples are tested and production-ready. Start with Quick Start if you're new to PHP LLM.</p>"},{"location":"examples/best-practices/","title":"Best Practices","text":"<p>Key patterns and principles for building robust, production-ready AI applications with PHP LLM.</p>"},{"location":"examples/best-practices/#caching","title":"Caching","text":""},{"location":"examples/best-practices/#why-caching-matters","title":"Why Caching Matters","text":"<p>Caching is essential for building efficient and cost-effective LLM applications:</p> <ul> <li>Cost savings: Eliminate redundant API calls for identical requests, reducing costs significantly</li> <li>Performance: Return cached responses instantly instead of waiting for API roundtrips</li> <li>Reliability: Reduce dependency on external API availability and rate limits</li> <li>Development efficiency: Speed up testing and development cycles with instant cached responses</li> </ul>"},{"location":"examples/best-practices/#when-to-use-caching","title":"When to Use Caching","text":"<p>Use caching when:</p> <ul> <li>You have repeated identical requests (same model, same conversation, same parameters)</li> <li>You're working in development/testing environments with repetitive queries</li> <li>Cost optimization is a priority for your application</li> <li>You have predictable query patterns that are likely to repeat</li> </ul>"},{"location":"examples/best-practices/#basic-implementation","title":"Basic Implementation","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\n\n// Enable caching for identical requests\n$cache = new FileCache(sys_get_temp_dir());\n$client = new AnthropicClient('sk-xxxxx', $cache);\n\n// Identical requests will automatically use cache, saving API calls\n</code></pre>"},{"location":"examples/best-practices/#learn-more","title":"Learn More","text":"<p>For detailed information on cache implementations, custom cache backends, cache warming strategies, and monitoring, see the comprehensive Caching Guide.</p>"},{"location":"examples/best-practices/#feedback-loops","title":"Feedback Loops","text":""},{"location":"examples/best-practices/#why-feedback-loops-matter","title":"Why Feedback Loops Matter","text":"<p>Feedback loops enable self-correcting AI agents that can validate and improve their own outputs:</p> <ul> <li>Validate outputs: Automatically check responses against quality criteria before accepting them</li> <li>Self-improve: Request corrections from the LLM without manual intervention</li> <li>Meet requirements: Ensure responses match your exact specifications (format, completeness, accuracy)</li> <li>Build reliability: Create consistent, validated outputs essential for production systems</li> </ul>"},{"location":"examples/best-practices/#when-to-use-feedback-loops","title":"When to Use Feedback Loops","text":"<p>Use feedback loops when:</p> <ul> <li>Output format validation is critical (JSON, XML, specific schemas)</li> <li>Content must meet specific criteria (length, completeness, accuracy requirements)</li> <li>You need guaranteed compliance with business rules</li> <li>Building agentic systems that must produce reliable, consistent results</li> </ul>"},{"location":"examples/best-practices/#key-principle-loop-counter","title":"Key Principle: Loop Counter","text":"<p>Always implement a loop counter to prevent infinite loops. This is a critical safeguard that prevents runaway costs and ensures your application remains responsive even when the LLM struggles to meet validation criteria.</p> <pre><code>&lt;?php\n$maxIterations = 5;\n$iteration = 0;\n\n$response = $chainClient-&gt;run(\n    client: $client,\n    request: new LLMRequest(\n        model: $model,\n        conversation: $conversation\n    ),\n    feedbackCallback: function (LLMResponse $response) use (&amp;$iteration, $maxIterations): ?LLMMessage {\n        $iteration++;\n\n        // CRITICAL: Stop after max attempts to prevent infinite loops\n        if ($iteration &gt;= $maxIterations) {\n            return null; // Stop iteration\n        }\n\n        // Your validation logic here\n        $text = $response-&gt;getLastText();\n        if (!isValidJson($text)) {\n            return LLMMessage::createFromUserString(\n                'The response was not valid JSON. Please provide a valid JSON response.'\n            );\n        }\n\n        return null; // Validation passed\n    }\n);\n</code></pre> <p>Without a loop counter, a feedback loop can continue indefinitely if the LLM cannot satisfy the validation criteria, leading to excessive API costs and application hangs.</p>"},{"location":"examples/best-practices/#learn-more_1","title":"Learn More","text":"<p>For complete examples of validation patterns, nested LLM validation, progressive feedback strategies, and combining feedback loops with tools, see the Feedback Loops Guide.</p>"},{"location":"examples/best-practices/#async-operations-for-parallel-tool-calls","title":"Async Operations for Parallel Tool Calls","text":""},{"location":"examples/best-practices/#why-async-operations-matter","title":"Why Async Operations Matter","text":"<p>Async operations are crucial for performance and efficiency in LLM applications:</p> <ul> <li>Performance: Process multiple requests concurrently instead of sequentially</li> <li>Efficiency: Reduce total execution time when handling multiple independent operations</li> <li>Scalability: Handle higher throughput with the same resources</li> <li>Tool calls: Execute multiple independent tool calls in parallel, dramatically speeding up agentic workflows</li> </ul>"},{"location":"examples/best-practices/#when-to-use-async-operations","title":"When to Use Async Operations","text":"<p>Use async operations when:</p> <ul> <li>You have multiple independent LLM requests to process</li> <li>Tool calls can be executed in parallel (no dependencies between them)</li> <li>Processing large batches of items</li> <li>Building real-time applications that need low latency</li> </ul>"},{"location":"examples/best-practices/#parallel-tool-call-pattern","title":"Parallel Tool Call Pattern","text":"<p>The most important use case for async operations is parallel tool execution. When an LLM agent needs to call multiple tools that don't depend on each other's results, async operations allow them to execute simultaneously rather than waiting for each to complete sequentially.</p> <p>For example, if an agent needs to fetch data from three different sources, running them in parallel can reduce execution time from 9 seconds (3 \u00d7 3 seconds) to just 3 seconds.</p> <pre><code>&lt;?php\n// Process multiple requests concurrently\n$promises = [];\n\nforeach ($items as $item) {\n    $promises[] = $chainClient-&gt;runAsync(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUserString(\"Analyze: {$item}\")\n            ])\n        )\n    );\n}\n\n// Wait for all to complete\n$responses = Promise\\Utils::all($promises)-&gt;wait();\n\n// Process results\nforeach ($responses as $response) {\n    echo $response-&gt;getLastText() . \"\\n\";\n}\n</code></pre>"},{"location":"examples/best-practices/#learn-more_2","title":"Learn More","text":"<p>For advanced async patterns, batch processing strategies, handling async tool execution, and error handling in concurrent operations, see:</p> <ul> <li>Tools &amp; Function Calling Guide - Tool implementation with async support</li> <li>Batch Processing Guide - Large-scale async operations</li> </ul>"},{"location":"examples/best-practices/#see-also","title":"See Also","text":"<ul> <li>Caching Guide - Comprehensive caching documentation</li> <li>Feedback Loops Guide - Building self-correcting agents</li> <li>Tools Guide - Function calling and tool usage</li> <li>Batch Processing Guide - High-volume processing</li> <li>State Management - Managing conversation state</li> </ul>"},{"location":"examples/logging-debugging/","title":"Logging &amp; Debugging","text":"<p>Monitor and debug your AI agents with built-in logging and debugging tools.</p>"},{"location":"examples/logging-debugging/#markdown-formatter","title":"Markdown Formatter","text":"<p>Convert requests and responses to readable markdown for logging or debugging:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\MarkdownFormatter;\n\n$formatter = new MarkdownFormatter();\n\n// Format response\n$markdown = $formatter-&gt;responseToMarkdown($response);\necho $markdown;\n\n// Format request\n$markdown = $formatter-&gt;requestToMarkdown($request);\necho $markdown;\n</code></pre> <p>Sample Output:</p> <pre><code>## Request\n**Model:** claude-sonnet-4-5-20250929\n**Temperature:** 1.0\n**Messages:** 2\n\n### User\nWhat is the capital of France?\n\n---\n\n## Response\n**Stop Reason:** end_turn\n**Input Tokens:** 15\n**Output Tokens:** 8\n**Cost:** $0.000345\n\n### Assistant\nThe capital of France is Paris.\n</code></pre>"},{"location":"examples/logging-debugging/#custom-logger","title":"Custom Logger","text":"<p>Implement <code>LLMLogger</code> for custom logging:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\LLMRequest;\nuse Soukicz\\Llm\\LLMResponse;\nuse Soukicz\\Llm\\Log\\LLMLogger;\nuse Soukicz\\Llm\\MarkdownFormatter;\n\nreadonly class LLMFileLogger implements LLMLogger {\n\n    public function __construct(\n        private string            $logPath,\n        private MarkdownFormatter $formatter\n    ) {\n    }\n\n    public function requestStarted(LLMRequest $request): void {\n        $markdown = $this-&gt;formatter-&gt;requestToMarkdown($request);\n        file_put_contents($this-&gt;logPath, $markdown . \"\\n\\n\", FILE_APPEND);\n    }\n\n    public function requestFinished(LLMResponse $response): void {\n        $markdown = $this-&gt;formatter-&gt;responseToMarkdown($response);\n        file_put_contents($this-&gt;logPath, $markdown . \"\\n\\n---\\n\\n\", FILE_APPEND);\n    }\n}\n</code></pre>"},{"location":"examples/logging-debugging/#using-the-logger","title":"Using the Logger","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\LLMChainClient;\nuse Soukicz\\Llm\\MarkdownFormatter;\n\n$logger = new LLMFileLogger(__DIR__ . '/llm.log', new MarkdownFormatter());\n$chainClient = new LLMChainClient($logger);\n\n// All requests will now be logged\n$response = $chainClient-&gt;run($client, $request);\n</code></pre>"},{"location":"examples/logging-debugging/#psr-3-logger-integration","title":"PSR-3 Logger Integration","text":"<p>Integrate with PSR-3 loggers (Monolog, etc.):</p> <pre><code>&lt;?php\nuse Psr\\Log\\LoggerInterface;\nuse Soukicz\\Llm\\Log\\LLMLogger;\n\nreadonly class PSR3LLMLogger implements LLMLogger {\n\n    public function __construct(\n        private LoggerInterface   $logger,\n        private MarkdownFormatter $formatter\n    ) {\n    }\n\n    public function requestStarted(LLMRequest $request): void {\n        $this-&gt;logger-&gt;info('LLM Request Started', [\n            'model' =&gt; $request-&gt;getModel()-&gt;getCode(),\n            'messages' =&gt; count($request-&gt;getConversation()-&gt;getMessages()),\n        ]);\n    }\n\n    public function requestFinished(LLMResponse $response): void {\n        $inputCost = $response-&gt;getInputPriceUsd() ?? 0;\n        $outputCost = $response-&gt;getOutputPriceUsd() ?? 0;\n\n        $this-&gt;logger-&gt;info('LLM Request Finished', [\n            'model' =&gt; $response-&gt;getRequest()-&gt;getModel()-&gt;getCode(),\n            'input_tokens' =&gt; $response-&gt;getInputTokens(),\n            'output_tokens' =&gt; $response-&gt;getOutputTokens(),\n            'cost' =&gt; $inputCost + $outputCost,\n            'response_time_ms' =&gt; $response-&gt;getTotalTimeMs(),\n        ]);\n    }\n}\n</code></pre> <pre><code>&lt;?php\nuse Monolog\\Logger;\nuse Monolog\\Handler\\StreamHandler;\n\n$monolog = new Logger('llm');\n$monolog-&gt;pushHandler(new StreamHandler(__DIR__ . '/llm.log', Logger::INFO));\n\n$logger = new PSR3LLMLogger($monolog, new MarkdownFormatter());\n$chainClient = new LLMChainClient($logger);\n</code></pre> <p>Sample Log Output:</p> <pre><code>[2025-01-15 10:23:45] llm.INFO: LLM Request Started {\"model\":\"claude-sonnet-4-5-20250929\",\"messages\":1}\n[2025-01-15 10:23:47] llm.INFO: LLM Request Finished {\"model\":\"claude-sonnet-4-5-20250929\",\"input_tokens\":15,\"output_tokens\":8,\"cost\":0.000345,\"response_time_ms\":1823}\n</code></pre>"},{"location":"examples/logging-debugging/#http-middleware-logging","title":"HTTP Middleware Logging","text":"<p>Log HTTP requests with Guzzle middleware:</p> <pre><code>&lt;?php\nuse GuzzleHttp\\HandlerStack;\nuse GuzzleHttp\\Middleware;\nuse GuzzleHttp\\MessageFormatter;\nuse Monolog\\Logger;\nuse Monolog\\Handler\\StreamHandler;\n\n$logger = new Logger('http');\n$logger-&gt;pushHandler(new StreamHandler(__DIR__ . '/http.log'));\n\n$stack = HandlerStack::create();\n$stack-&gt;push(\n    Middleware::log(\n        $logger,\n        new MessageFormatter('{method} {uri} - {code} - {res_body}')\n    )\n);\n\n$client = new AnthropicClient(\n    apiKey: 'sk-xxxxx',\n    cache: $cache,\n    handler: $stack\n);\n</code></pre>"},{"location":"examples/logging-debugging/#debugging-failed-requests","title":"Debugging Failed Requests","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\LLMClientException;\n\ntry {\n    $response = $chainClient-&gt;run($client, $request);\n} catch (LLMClientException $e) {\n    // Log error details\n    error_log(\"LLM Error: \" . $e-&gt;getMessage());\n    error_log(\"Request: \" . $formatter-&gt;requestToMarkdown($request));\n\n    // Check if it's a rate limit\n    if ($e-&gt;getCode() === 429) {\n        sleep(60);\n        $response = $chainClient-&gt;run($client, $request); // Retry\n    }\n}\n</code></pre>"},{"location":"examples/logging-debugging/#performance-monitoring","title":"Performance Monitoring","text":"<p>Track request timing and costs:</p> <pre><code>&lt;?php\nclass PerformanceLogger implements LLMLogger {\n    private array $timings = [];\n\n    public function requestStarted(LLMRequest $request): void {\n        $this-&gt;timings[spl_object_id($request)] = microtime(true);\n    }\n\n    public function requestFinished(LLMResponse $response): void {\n        $requestId = spl_object_id($response-&gt;getRequest());\n        $duration = isset($this-&gt;timings[$requestId])\n            ? (microtime(true) - $this-&gt;timings[$requestId]) * 1000\n            : $response-&gt;getTotalTimeMs();\n\n        $totalTokens = $response-&gt;getInputTokens() + $response-&gt;getOutputTokens();\n        $totalCost = ($response-&gt;getInputPriceUsd() ?? 0) + ($response-&gt;getOutputPriceUsd() ?? 0);\n\n        echo sprintf(\n            \"Request %s: %dms, %d tokens, $%.6f\\n\",\n            $response-&gt;getRequest()-&gt;getModel()-&gt;getCode(),\n            $duration,\n            $totalTokens,\n            $totalCost\n        );\n\n        unset($this-&gt;timings[$requestId]);\n    }\n}\n</code></pre> <p>Sample Output:</p> <pre><code>Request claude-sonnet-4-5-20250929: 1823ms, 23 tokens, $0.000345\nRequest gpt-5-2025-08-07: 956ms, 45 tokens, $0.000890\nRequest gemini-2.5-pro: 1245ms, 31 tokens, $0.000520\n</code></pre>"},{"location":"examples/logging-debugging/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose debugging to inspect all request/response details:</p> <pre><code>&lt;?php\nclass DebugLogger implements LLMLogger {\n    public function requestStarted(LLMRequest $request): void {\n        echo \"=== REQUEST STARTED ===\\n\";\n        echo \"Model: \" . $request-&gt;getModel()-&gt;getCode() . \"\\n\";\n        echo \"Temperature: \" . ($request-&gt;getTemperature() ?? 'default') . \"\\n\";\n        echo \"Max Tokens: \" . ($request-&gt;getMaxTokens() ?? 'default') . \"\\n\";\n        echo \"Messages: \" . count($request-&gt;getConversation()-&gt;getMessages()) . \"\\n\";\n        echo \"Tools: \" . count($request-&gt;getTools()) . \"\\n\\n\";\n    }\n\n    public function requestFinished(LLMResponse $response): void {\n        echo \"=== REQUEST FINISHED ===\\n\";\n        echo \"Stop Reason: \" . $response-&gt;getStopReason() . \"\\n\";\n        echo \"Response Time: \" . $response-&gt;getTotalTimeMs() . \"ms\\n\";\n        echo \"Input Tokens: \" . $response-&gt;getInputTokens() . \"\\n\";\n        echo \"Output Tokens: \" . $response-&gt;getOutputTokens() . \"\\n\";\n\n        $totalCost = ($response-&gt;getInputPriceUsd() ?? 0) + ($response-&gt;getOutputPriceUsd() ?? 0);\n        echo \"Cost: $\" . number_format($totalCost, 6) . \"\\n\";\n\n        $text = $response-&gt;getLastText();\n        echo \"Response: \" . substr($text, 0, 100) . (strlen($text) &gt; 100 ? \"...\" : \"\") . \"\\n\\n\";\n    }\n}\n</code></pre> <p>Sample Output:</p> <pre><code>=== REQUEST STARTED ===\nModel: claude-sonnet-4-5-20250929\nTemperature: 1.0\nMax Tokens: 2048\nMessages: 1\nTools: 0\n\n=== REQUEST FINISHED ===\nStop Reason: end_turn\nResponse Time: 1823ms\nInput Tokens: 15\nOutput Tokens: 8\nCost: $0.000345\nResponse: The capital of France is Paris.\n</code></pre>"},{"location":"examples/logging-debugging/#structured-logging","title":"Structured Logging","text":"<p>Log in JSON format for analysis and monitoring:</p> <pre><code>&lt;?php\nclass JSONLogger implements LLMLogger {\n    public function __construct(private string $logFile) {}\n\n    public function requestStarted(LLMRequest $request): void {\n        // Optional: log request start\n    }\n\n    public function requestFinished(LLMResponse $response): void {\n        $inputCost = $response-&gt;getInputPriceUsd() ?? 0;\n        $outputCost = $response-&gt;getOutputPriceUsd() ?? 0;\n\n        $log = [\n            'timestamp' =&gt; date('c'),\n            'model' =&gt; $response-&gt;getRequest()-&gt;getModel()-&gt;getCode(),\n            'input_tokens' =&gt; $response-&gt;getInputTokens(),\n            'output_tokens' =&gt; $response-&gt;getOutputTokens(),\n            'total_tokens' =&gt; $response-&gt;getInputTokens() + $response-&gt;getOutputTokens(),\n            'input_cost' =&gt; $inputCost,\n            'output_cost' =&gt; $outputCost,\n            'total_cost' =&gt; $inputCost + $outputCost,\n            'response_time_ms' =&gt; $response-&gt;getTotalTimeMs(),\n            'stop_reason' =&gt; $response-&gt;getStopReason(),\n        ];\n\n        file_put_contents(\n            $this-&gt;logFile,\n            json_encode($log) . \"\\n\",\n            FILE_APPEND\n        );\n    }\n}\n</code></pre> <p>Sample Log Output (llm.json):</p> <pre><code>{\"timestamp\":\"2025-01-15T10:23:47+00:00\",\"model\":\"claude-sonnet-4-5-20250929\",\"input_tokens\":15,\"output_tokens\":8,\"total_tokens\":23,\"input_cost\":0.000045,\"output_cost\":0.0003,\"total_cost\":0.000345,\"response_time_ms\":1823,\"stop_reason\":\"end_turn\"}\n{\"timestamp\":\"2025-01-15T10:24:12+00:00\",\"model\":\"gpt-5-2025-08-07\",\"input_tokens\":22,\"output_tokens\":45,\"total_tokens\":67,\"input_cost\":0.00011,\"output_cost\":0.00078,\"total_cost\":0.00089,\"response_time_ms\":956,\"stop_reason\":\"stop\"}\n{\"timestamp\":\"2025-01-15T10:25:03+00:00\",\"model\":\"gemini-2.5-pro\",\"input_tokens\":18,\"output_tokens\":31,\"total_tokens\":49,\"input_cost\":0.00009,\"output_cost\":0.00043,\"total_cost\":0.00052,\"response_time_ms\":1245,\"stop_reason\":\"STOP\"}\n</code></pre> <p>This format is ideal for log aggregation tools like ELK stack, Splunk, or DataDog.</p>"},{"location":"examples/logging-debugging/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - Configure logging behavior</li> <li>Quick Start - Basic usage examples</li> </ul>"},{"location":"examples/multimodal/","title":"Multimodal Examples","text":"<p>Practical examples for processing images and PDFs with AI models. Perfect for document analysis, visual understanding, and data extraction tasks.</p>"},{"location":"examples/multimodal/#image-analysis-examples","title":"Image Analysis Examples","text":""},{"location":"examples/multimodal/#product-image-description-generator","title":"Product Image Description Generator","text":"<p>Generate SEO-friendly product descriptions from images:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\nuse Soukicz\\Llm\\Message\\LLMMessageImage;\nuse Soukicz\\Llm\\Message\\LLMMessageText;\n\nfunction generateProductDescription(string $imagePath): string {\n    global $chainClient, $client, $model;\n\n    $imageData = base64_encode(file_get_contents($imagePath));\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents([\n                    new LLMMessageText(\n                        'Create a detailed product description for this item. ' .\n                        'Include: product type, colors, materials, style, and key features. ' .\n                        'Write in an engaging, SEO-friendly style.'\n                    ),\n                    new LLMMessageImage('base64', 'image/jpeg', $imageData)\n                ]))\n            ])\n        )\n    );\n\n    return $response-&gt;getLastText();\n}\n\n// Usage\n$description = generateProductDescription('/path/to/product.jpg');\necho $description;\n</code></pre> <p>Sample Output:</p> <pre><code>Elegant Modern Leather Sofa\n\nThis stunning three-seater sofa features premium full-grain leather upholstery in a rich\ncognac brown finish. The mid-century modern design includes clean lines, tapered wooden\nlegs, and tufted cushioning for both style and comfort...\n</code></pre>"},{"location":"examples/multimodal/#screenshot-debugging-assistant","title":"Screenshot Debugging Assistant","text":"<p>Analyze UI screenshots for bugs and improvements:</p> <pre><code>&lt;?php\nfunction analyzeUIScreenshot(string $screenshotPath): array {\n    global $chainClient, $client, $model;\n\n    $imageData = base64_encode(file_get_contents($screenshotPath));\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents([\n                    new LLMMessageText(\n                        'Analyze this UI screenshot and provide:\\n' .\n                        '1. Accessibility issues (contrast, font sizes, etc.)\\n' .\n                        '2. Layout problems (alignment, spacing, overlapping)\\n' .\n                        '3. Responsive design concerns\\n' .\n                        '4. UX improvement suggestions\\n\\n' .\n                        'Format as a structured list with severity levels.'\n                    ),\n                    new LLMMessageImage('base64', 'image/png', $imageData)\n                ]))\n            ])\n        )\n    );\n\n    return [\n        'analysis' =&gt; $response-&gt;getLastText(),\n        'screenshot' =&gt; $screenshotPath,\n        'analyzed_at' =&gt; date('c')\n    ];\n}\n</code></pre>"},{"location":"examples/multimodal/#chart-and-graph-data-extraction","title":"Chart and Graph Data Extraction","text":"<p>Extract data from visualization images:</p> <pre><code>&lt;?php\nfunction extractChartData(string $chartImagePath): array {\n    global $chainClient, $client, $model;\n\n    $imageData = base64_encode(file_get_contents($chartImagePath));\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents([\n                    new LLMMessageText(\n                        'Extract all data points from this chart/graph and return them as JSON. ' .\n                        'Include labels, values, and any legends or annotations. ' .\n                        'Format: {\"type\": \"bar|line|pie\", \"data\": [{...}], \"title\": \"...\", \"axes\": {...}}'\n                    ),\n                    new LLMMessageImage('base64', 'image/png', $imageData)\n                ]))\n            ])\n        )\n    );\n\n    return json_decode($response-&gt;getLastText(), true);\n}\n\n// Usage\n$chartData = extractChartData('/path/to/sales-chart.png');\nprint_r($chartData);\n</code></pre>"},{"location":"examples/multimodal/#receipt-and-invoice-processing","title":"Receipt and Invoice Processing","text":"<pre><code>&lt;?php\nfunction processReceipt(string $receiptImagePath): array {\n    global $chainClient, $client, $model;\n\n    $imageData = base64_encode(file_get_contents($receiptImagePath));\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents([\n                    new LLMMessageText(\n                        'Extract structured data from this receipt. Return JSON with: ' .\n                        'merchant_name, date, total, tax, items (name, quantity, price), payment_method'\n                    ),\n                    new LLMMessageImage('base64', 'image/jpeg', $imageData)\n                ]))\n            ])\n        )\n    );\n\n    return json_decode($response-&gt;getLastText(), true);\n}\n</code></pre> <p>Sample Usage:</p> <pre><code>&lt;?php\n$receipt = processReceipt('/uploads/receipt-001.jpg');\n\n// Store in database\n$pdo-&gt;prepare('INSERT INTO expenses (merchant, date, amount, tax, items) VALUES (?, ?, ?, ?, ?)')\n    -&gt;execute([\n        $receipt['merchant_name'],\n        $receipt['date'],\n        $receipt['total'],\n        $receipt['tax'],\n        json_encode($receipt['items'])\n    ]);\n</code></pre>"},{"location":"examples/multimodal/#pdf-analysis-examples","title":"PDF Analysis Examples","text":""},{"location":"examples/multimodal/#contract-review-assistant","title":"Contract Review Assistant","text":"<pre><code>&lt;?php\nfunction reviewContract(string $contractPdfPath): array {\n    global $chainClient, $client, $model;\n\n    $pdfData = base64_encode(file_get_contents($contractPdfPath));\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents([\n                    new LLMMessageText(\n                        'Review this contract and provide:\\n' .\n                        '1. Key terms (parties, dates, amounts)\\n' .\n                        '2. Obligations and responsibilities\\n' .\n                        '3. Termination clauses\\n' .\n                        '4. Potential red flags or unusual clauses\\n' .\n                        '5. Missing standard clauses\\n\\n' .\n                        'Format as a structured report.'\n                    ),\n                    new LLMMessagePdf('base64', $pdfData)\n                ]))\n            ])\n        )\n    );\n\n    return [\n        'summary' =&gt; $response-&gt;getLastText(),\n        'filename' =&gt; basename($contractPdfPath),\n        'reviewed_at' =&gt; date('c'),\n        'tokens_used' =&gt; $response-&gt;getInputTokens() + $response-&gt;getOutputTokens()\n    ];\n}\n</code></pre>"},{"location":"examples/multimodal/#research-paper-summarizer","title":"Research Paper Summarizer","text":"<pre><code>&lt;?php\nfunction summarizeResearchPaper(string $paperPdfPath): string {\n    global $chainClient, $client, $model;\n\n    $pdfData = base64_encode(file_get_contents($paperPdfPath));\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents([\n                    new LLMMessageText(\n                        'Summarize this research paper. Include:\\n' .\n                        '- Research question/hypothesis\\n' .\n                        '- Methodology\\n' .\n                        '- Key findings\\n' .\n                        '- Conclusions\\n' .\n                        '- Limitations\\n\\n' .\n                        'Write for a technical but non-specialist audience (max 500 words).'\n                    ),\n                    new LLMMessagePdf('base64', $pdfData)\n                ]))\n            ])\n        )\n    );\n\n    return $response-&gt;getLastText();\n}\n</code></pre>"},{"location":"examples/multimodal/#multi-document-comparison","title":"Multi-Document Comparison","text":"<pre><code>&lt;?php\nfunction compareDocuments(string $pdf1Path, string $pdf2Path): string {\n    global $chainClient, $client, $model;\n\n    $pdf1Data = base64_encode(file_get_contents($pdf1Path));\n    $pdf2Data = base64_encode(file_get_contents($pdf2Path));\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents([\n                    new LLMMessageText('Compare these two documents and highlight:'),\n                    new LLMMessagePdf('base64', $pdf1Data),\n                    new LLMMessageText('vs'),\n                    new LLMMessagePdf('base64', $pdf2Data),\n                    new LLMMessageText(\n                        'Key differences, additions, removals, and modifications. ' .\n                        'Focus on substantive changes, not formatting.'\n                    )\n                ]))\n            ])\n        )\n    );\n\n    return $response-&gt;getLastText();\n}\n\n// Usage: Compare contract versions\n$diff = compareDocuments(\n    '/contracts/version1.pdf',\n    '/contracts/version2.pdf'\n);\n</code></pre>"},{"location":"examples/multimodal/#table-extraction-from-pdfs","title":"Table Extraction from PDFs","text":"<pre><code>&lt;?php\nfunction extractTablesFromPdf(string $pdfPath): array {\n    global $chainClient, $client, $model;\n\n    $pdfData = base64_encode(file_get_contents($pdfPath));\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents([\n                    new LLMMessageText(\n                        'Extract all tables from this PDF. ' .\n                        'Return each table as a JSON array with headers and rows. ' .\n                        'Format: [{\"table_number\": 1, \"title\": \"...\", \"headers\": [...], \"rows\": [[...]]}]'\n                    ),\n                    new LLMMessagePdf('base64', $pdfData)\n                ]))\n            ])\n        )\n    );\n\n    return json_decode($response-&gt;getLastText(), true);\n}\n\n// Convert to CSV\n$tables = extractTablesFromPdf('/reports/annual-report.pdf');\nforeach ($tables as $i =&gt; $table) {\n    $csv = fopen(\"table_{$i}.csv\", 'w');\n    fputcsv($csv, $table['headers']);\n    foreach ($table['rows'] as $row) {\n        fputcsv($csv, $row);\n    }\n    fclose($csv);\n}\n</code></pre>"},{"location":"examples/multimodal/#mixed-media-examples","title":"Mixed Media Examples","text":""},{"location":"examples/multimodal/#document-analysis-with-supporting-images","title":"Document Analysis with Supporting Images","text":"<pre><code>&lt;?php\nfunction analyzeProposal(string $proposalPdf, array $mockupImagePaths): array {\n    global $chainClient, $client, $model;\n\n    $pdfData = base64_encode(file_get_contents($proposalPdf));\n\n    // Build contents array\n    $contents = [\n        new LLMMessageText('Review this proposal document and the accompanying design mockups:'),\n        new LLMMessagePdf('base64', $pdfData),\n    ];\n\n    foreach ($mockupImagePaths as $i =&gt; $imagePath) {\n        $imageData = base64_encode(file_get_contents($imagePath));\n        $contents[] = new LLMMessageText(\"Design Mockup \" . ($i + 1) . \":\");\n        $contents[] = new LLMMessageImage('base64', 'image/png', $imageData);\n    }\n\n    $contents[] = new LLMMessageText(\n        'Provide: 1) Proposal evaluation, 2) Design mockup analysis, ' .\n        '3) Alignment between proposal and designs, 4) Recommendations'\n    );\n\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUser(new LLMMessageContents($contents))\n            ])\n        )\n    );\n\n    return [\n        'evaluation' =&gt; $response-&gt;getLastText(),\n        'cost' =&gt; ($response-&gt;getInputPriceUsd() ?? 0) + ($response-&gt;getOutputPriceUsd() ?? 0),\n        'tokens' =&gt; $response-&gt;getInputTokens() + $response-&gt;getOutputTokens()\n    ];\n}\n</code></pre>"},{"location":"examples/multimodal/#performance-and-cost-optimization","title":"Performance and Cost Optimization","text":""},{"location":"examples/multimodal/#image-compression","title":"Image Compression","text":"<p>Reduce costs by compressing images before sending:</p> <pre><code>&lt;?php\nfunction compressImage(string $imagePath, int $maxWidth = 1024): string {\n    $image = imagecreatefromstring(file_get_contents($imagePath));\n    $width = imagesx($image);\n    $height = imagesy($image);\n\n    if ($width &gt; $maxWidth) {\n        $newWidth = $maxWidth;\n        $newHeight = (int)(($height / $width) * $maxWidth);\n\n        $resized = imagecreatetruecolor($newWidth, $newHeight);\n        imagecopyresampled($resized, $image, 0, 0, 0, 0, $newWidth, $newHeight, $width, $height);\n\n        ob_start();\n        imagejpeg($resized, null, 85); // 85% quality\n        $compressed = ob_get_clean();\n\n        imagedestroy($image);\n        imagedestroy($resized);\n\n        return base64_encode($compressed);\n    }\n\n    return base64_encode(file_get_contents($imagePath));\n}\n\n// Usage\n$imageData = compressImage('/path/to/large-image.jpg', 1024);\n</code></pre>"},{"location":"examples/multimodal/#caching-multimodal-requests","title":"Caching Multimodal Requests","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\n\n$cache = new FileCache(sys_get_temp_dir());\n\n// Identical multimodal requests will use cache\n// Saves API costs for repeated analyses\n$client = new AnthropicClient('sk-xxxxx', $cache);\n</code></pre>"},{"location":"examples/multimodal/#prompt-caching-for-large-documents","title":"Prompt Caching for Large Documents","text":"<p>Use Claude's prompt caching to reduce costs for repeated PDF analysis:</p> <pre><code>&lt;?php\n// Mark large PDFs as cacheable\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('Analyze this document:'),\n    new LLMMessagePdf('base64', $pdfData, cached: true)  // Cache this PDF\n]));\n\n// Subsequent requests with the same PDF will use cached version\n// Reduces costs by ~90% for the cached portion\n</code></pre>"},{"location":"examples/multimodal/#error-handling","title":"Error Handling","text":""},{"location":"examples/multimodal/#file-size-validation","title":"File Size Validation","text":"<pre><code>&lt;?php\nfunction validateFileSize(string $filePath, int $maxSizeMB = 10): void {\n    $sizeMB = filesize($filePath) / 1024 / 1024;\n\n    if ($sizeMB &gt; $maxSizeMB) {\n        throw new Exception(\n            \"File too large: {$sizeMB}MB (max {$maxSizeMB}MB). \" .\n            \"Consider compressing or splitting the file.\"\n        );\n    }\n}\n\n// Usage\ntry {\n    validateFileSize($imagePath, 5);\n    $imageData = base64_encode(file_get_contents($imagePath));\n    // ... process image\n} catch (Exception $e) {\n    error_log($e-&gt;getMessage());\n}\n</code></pre>"},{"location":"examples/multimodal/#provider-compatibility-check","title":"Provider Compatibility Check","text":"<pre><code>&lt;?php\nfunction supportsMultimodal($model): bool {\n    // Check if model supports images/PDFs\n    return $model instanceof AnthropicClaude45Sonnet ||\n           $model instanceof GPT5 ||\n           $model instanceof Gemini25Pro;\n}\n\nif (!supportsMultimodal($model)) {\n    throw new Exception('Selected model does not support multimodal input');\n}\n</code></pre>"},{"location":"examples/multimodal/#best-practices","title":"Best Practices","text":"<ol> <li>Compress images before sending to reduce costs and latency</li> <li>Use caching for repeated analyses of the same documents</li> <li>Validate file sizes to avoid API errors</li> <li>Choose the right format: JPEG for photos, PNG for screenshots/diagrams</li> <li>Clear prompts: Specify exactly what you want extracted or analyzed</li> <li>Structure outputs: Request JSON for easy parsing and storage</li> <li>Monitor costs: Track tokens for multimodal requests (images use more tokens)</li> </ol>"},{"location":"examples/multimodal/#see-also","title":"See Also","text":"<ul> <li>Multimodal Guide - Technical details and provider support</li> <li>Tools &amp; Function Calling - Combine tools with multimodal input</li> <li>State Management - Saving multimodal conversations</li> </ul>"},{"location":"examples/quick-start/","title":"Quick Start Examples","text":"<p>Get started with PHP LLM in minutes with these basic examples.</p>"},{"location":"examples/quick-start/#installation","title":"Installation","text":"<pre><code>composer require soukicz/llm\n</code></pre>"},{"location":"examples/quick-start/#simple-synchronous-request","title":"Simple Synchronous Request","text":"<p>The most basic way to interact with an LLM. This pattern is perfect for simple one-off requests where you don't need conversation history.</p> <pre><code>&lt;?php\nrequire_once __DIR__ . '/vendor/autoload.php';\n\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\nuse Soukicz\\Llm\\Client\\LLMChainClient;\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\n\n// Set up caching to avoid redundant API calls\n$cache = new FileCache(sys_get_temp_dir());\n\n// Initialize the Anthropic client with your API key\n$anthropic = new AnthropicClient('sk-xxxxx', $cache);\n\n// The chain client handles the request/response cycle\n$chainClient = new LLMChainClient();\n\n// Make a synchronous request\n$response = $chainClient-&gt;run(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString('What is PHP?')\n        ]),\n    )\n);\n\n// Get the AI's response text\necho $response-&gt;getLastText();\n</code></pre>"},{"location":"examples/quick-start/#async-request","title":"Async Request","text":"<p>Use asynchronous requests when you need to make multiple LLM calls concurrently or when you want non-blocking execution. This uses promises under the hood.</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\LLMResponse;\n\n// Start an async request (doesn't block)\n$promise = $chainClient-&gt;runAsync(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString('Explain async programming')\n        ]),\n    )\n);\n\n// Handle the response when it arrives\n$promise-&gt;then(function (LLMResponse $response) {\n    echo $response-&gt;getLastText();\n});\n\n// You can do other work here while waiting for the response\n// Or make multiple async requests and wait for all to complete\n</code></pre>"},{"location":"examples/quick-start/#multi-turn-conversation","title":"Multi-Turn Conversation","text":"<p>This library uses immutable objects - methods like <code>withMessage()</code> return a new instance rather than modifying the original. This prevents accidental state mutations and makes your code more predictable.</p> <pre><code>&lt;?php\n$conversation = new LLMConversation([\n    LLMMessage::createFromUserString('What is 2 + 2?'),\n]);\n\n// First turn\n$response = $chainClient-&gt;run(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: $conversation,\n    )\n);\n\necho \"AI: \" . $response-&gt;getLastText() . \"\\n\"; // \"4\"\n\n// Add AI response to conversation (returns new instance)\n$conversation = $conversation-&gt;withMessage($response-&gt;getLastMessage());\n\n// Add user's follow-up question (returns new instance)\n$conversation = $conversation-&gt;withMessage(\n    LLMMessage::createFromUserString('What about 2 * 2?')\n);\n\n// Second turn\n$response = $chainClient-&gt;run(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: $conversation,\n    )\n);\n\necho \"AI: \" . $response-&gt;getLastText() . \"\\n\"; // \"4\"\n</code></pre>"},{"location":"examples/quick-start/#different-providers","title":"Different Providers","text":"<p>PHP LLM provides a unified interface across multiple LLM providers. Simply swap the client and model - the rest of your code stays the same.</p>"},{"location":"examples/quick-start/#openai","title":"OpenAI","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAIClient;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPT5;\n\n$openai = new OpenAIClient('sk-xxxxx', 'org-xxxxx', $cache);\n\n$response = $chainClient-&gt;run(\n    client: $openai,\n    request: new LLMRequest(\n        model: new GPT5(GPT5::VERSION_2025_08_07),\n        conversation: $conversation,\n    )\n);\n</code></pre>"},{"location":"examples/quick-start/#google-gemini","title":"Google Gemini","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\Gemini\\GeminiClient;\nuse Soukicz\\Llm\\Client\\Gemini\\Model\\Gemini25Pro;\n\n$gemini = new GeminiClient('your-api-key', $cache);\n\n$response = $chainClient-&gt;run(\n    client: $gemini,\n    request: new LLMRequest(\n        model: new Gemini25Pro(),\n        conversation: $conversation,\n    )\n);\n</code></pre>"},{"location":"examples/quick-start/#openrouter","title":"OpenRouter","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAICompatibleClient;\nuse Soukicz\\Llm\\Client\\Universal\\LocalModel;\n\n$client = new OpenAICompatibleClient(\n    apiKey: 'sk-or-v1-xxxxx',\n    baseUrl: 'https://openrouter.ai/api/v1',\n);\n\n$response = $chainClient-&gt;run(\n    client: $client,\n    request: new LLMRequest(\n        model: new LocalModel('anthropic/claude-3.5-sonnet'),\n        conversation: $conversation,\n    )\n);\n</code></pre>"},{"location":"examples/quick-start/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>&lt;?php\n$anthropic = new AnthropicClient(\n    apiKey: getenv('ANTHROPIC_API_KEY'),\n    cache: $cache\n);\n\n$openai = new OpenAIClient(\n    apiKey: getenv('OPENAI_API_KEY'),\n    apiOrganization: getenv('OPENAI_ORG_ID'),\n    cache: $cache\n);\n\n$gemini = new GeminiClient(\n    apiKey: getenv('GEMINI_API_KEY'),\n    cache: $cache\n);\n</code></pre>"},{"location":"examples/quick-start/#error-handling","title":"Error Handling","text":"<p>All client operations can throw <code>LLMClientException</code> for API errors, network failures, and invalid responses. Always wrap your LLM calls in try-catch blocks for production code.</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\LLMClientException;\n\ntry {\n    $response = $chainClient-&gt;run($client, $request);\n    echo $response-&gt;getLastText();\n} catch (LLMClientException $e) {\n    echo \"Error: \" . $e-&gt;getMessage();\n    // Handle error: log, retry, fallback, etc.\n}\n</code></pre>"},{"location":"examples/quick-start/#tracking-costs","title":"Tracking Costs","text":"<p>Every response includes token usage and cost information. This helps you monitor API expenses and optimize your prompts.</p> <pre><code>&lt;?php\n$response = $chainClient-&gt;run($client, $request);\n\n// Token counts\necho \"Input tokens: \" . $response-&gt;getInputTokens() . \"\\n\";\necho \"Output tokens: \" . $response-&gt;getOutputTokens() . \"\\n\";\necho \"Total tokens: \" . ($response-&gt;getInputTokens() + $response-&gt;getOutputTokens()) . \"\\n\";\n\n// Cost breakdown (in USD, null if pricing unavailable)\n$inputCost = $response-&gt;getInputPriceUsd() ?? 0;\n$outputCost = $response-&gt;getOutputPriceUsd() ?? 0;\n$totalCost = $inputCost + $outputCost;\n\necho \"Input cost: $\" . number_format($inputCost, 6) . \"\\n\";\necho \"Output cost: $\" . number_format($outputCost, 6) . \"\\n\";\necho \"Total cost: $\" . number_format($totalCost, 6) . \"\\n\";\n\n// Performance metrics\necho \"Response time: \" . $response-&gt;getTotalTimeMs() . \"ms\\n\";\n</code></pre>"},{"location":"examples/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Tools Guide - Add function calling to your agents</li> <li>Multimodal - Process images and PDFs</li> <li>Feedback Loops - Build self-correcting agents</li> <li>Configuration - Advanced configuration options</li> </ul>"},{"location":"examples/state-management/","title":"State Management","text":"<p>Save and resume AI agent conversations using JSON serialization. Conversation state management is essential for building chatbots, customer support systems, and any application where context needs to persist across multiple requests.</p>"},{"location":"examples/state-management/#key-concepts","title":"Key Concepts","text":"<p>Serialization: Conversations can be serialized to JSON and stored in files, databases, Redis, or sessions. The library uses <code>JsonSerializable</code> for automatic encoding.</p> <p>Immutability: Remember that <code>LLMConversation</code> is immutable - use <code>withMessage()</code> to add messages, which returns a new instance.</p>"},{"location":"examples/state-management/#saving-conversations","title":"Saving Conversations","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\Message\\LLMMessage;\n\n$conversation = new LLMConversation([\n    LLMMessage::createFromUserString('Hello'),\n    LLMMessage::createFromAssistantString('Hi! How can I help?'),\n    LLMMessage::createFromUserString('Tell me about PHP'),\n]);\n\n// Serialize to JSON\n$json = json_encode($conversation);\n\n// Save to file\nfile_put_contents('conversation.json', $json);\n</code></pre>"},{"location":"examples/state-management/#loading-conversations","title":"Loading Conversations","text":"<pre><code>&lt;?php\n// Load from file\n$json = file_get_contents('conversation.json');\n\n// Deserialize (fromJson expects an array, not JSON string)\n$conversation = LLMConversation::fromJson(json_decode($json, true));\n\n// Continue the conversation (remember: immutable objects!)\n$conversation = $conversation-&gt;withMessage(\n    LLMMessage::createFromUserString('What are traits?')\n);\n\n$response = $chainClient-&gt;run(\n    client: $client,\n    request: new LLMRequest(\n        model: $model,\n        conversation: $conversation,\n    )\n);\n</code></pre>"},{"location":"examples/state-management/#database-storage","title":"Database Storage","text":"<p>Store conversations in a relational database for persistent multi-user applications. This approach provides ACID guarantees and supports complex queries.</p> <p>Database Schema Example:</p> <pre><code>CREATE TABLE conversations (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    user_id VARCHAR(255) NOT NULL,\n    data JSON NOT NULL,\n    created_at DATETIME NOT NULL,\n    updated_at DATETIME NOT NULL,\n    active BOOLEAN DEFAULT 1,\n    INDEX idx_user_active (user_id, active, updated_at)\n);\n</code></pre>"},{"location":"examples/state-management/#save-to-database","title":"Save to Database","text":"<pre><code>&lt;?php\nuse PDO;\n\nfunction saveConversation(PDO $pdo, string $userId, LLMConversation $conversation): int {\n    $stmt = $pdo-&gt;prepare('\n        INSERT INTO conversations (user_id, data, created_at)\n        VALUES (:user_id, :data, NOW())\n    ');\n\n    $stmt-&gt;execute([\n        'user_id' =&gt; $userId,\n        'data' =&gt; json_encode($conversation),\n    ]);\n\n    return $pdo-&gt;lastInsertId();\n}\n</code></pre>"},{"location":"examples/state-management/#load-from-database","title":"Load from Database","text":"<pre><code>&lt;?php\nfunction loadConversation(PDO $pdo, int $conversationId): ?LLMConversation {\n    $stmt = $pdo-&gt;prepare('\n        SELECT data FROM conversations WHERE id = :id\n    ');\n\n    $stmt-&gt;execute(['id' =&gt; $conversationId]);\n    $data = $stmt-&gt;fetchColumn();\n\n    return $data ? LLMConversation::fromJson(json_decode($data, true)) : null;\n}\n</code></pre>"},{"location":"examples/state-management/#redis-storage","title":"Redis Storage","text":"<p>Use Redis for fast, temporary conversation storage. Perfect for high-traffic applications where you need quick access but don't require permanent storage. The TTL (Time To Live) automatically expires old conversations.</p> <pre><code>&lt;?php\nuse Redis;\n\nclass ConversationStore {\n    public function __construct(private Redis $redis) {}\n\n    public function save(string $key, LLMConversation $conversation): void {\n        $this-&gt;redis-&gt;setex(\n            $key,\n            3600, // 1 hour TTL\n            json_encode($conversation)\n        );\n    }\n\n    public function load(string $key): ?LLMConversation {\n        $json = $this-&gt;redis-&gt;get($key);\n        return $json ? LLMConversation::fromJson(json_decode($json, true)) : null;\n    }\n\n    public function delete(string $key): void {\n        $this-&gt;redis-&gt;del($key);\n    }\n}\n</code></pre> <pre><code>&lt;?php\n$store = new ConversationStore($redis);\n\n// Save\n$store-&gt;save(\"user:{$userId}:conversation\", $conversation);\n\n// Load\n$conversation = $store-&gt;load(\"user:{$userId}:conversation\");\n\n// Delete\n$store-&gt;delete(\"user:{$userId}:conversation\");\n</code></pre>"},{"location":"examples/state-management/#session-storage","title":"Session Storage","text":"<p>Store conversations in PHP sessions for simple, single-server applications. This is the easiest approach for getting started but doesn't work well with load balancers or distributed systems.</p> <pre><code>&lt;?php\nsession_start();\n\n// Save to session\n$_SESSION['conversation'] = json_encode($conversation);\n\n// Load from session\n$conversation = isset($_SESSION['conversation'])\n    ? LLMConversation::fromJson(json_decode($_SESSION['conversation'], true))\n    : new LLMConversation();\n</code></pre>"},{"location":"examples/state-management/#conversation-history-management","title":"Conversation History Management","text":"<p>Long conversations can exceed model context windows and become expensive. Implement strategies to manage conversation size.</p> <p>Why trim conversations? - Models have token limits (e.g., 200K for Claude, 128K for GPT-4) - Longer context = higher costs - Very long contexts can degrade performance</p>"},{"location":"examples/state-management/#limit-history-length","title":"Limit History Length","text":"<pre><code>&lt;?php\nfunction trimConversation(LLMConversation $conversation, int $maxMessages): LLMConversation {\n    $messages = $conversation-&gt;getMessages();\n\n    if (count($messages) &lt;= $maxMessages) {\n        return $conversation;\n    }\n\n    // Keep most recent messages\n    $trimmedMessages = array_slice($messages, -$maxMessages);\n\n    return new LLMConversation($trimmedMessages);\n}\n\n$conversation = trimConversation($conversation, 20); // Keep last 20 messages\n</code></pre>"},{"location":"examples/state-management/#conversation-metadata","title":"Conversation Metadata","text":"<pre><code>&lt;?php\nclass ConversationMetadata {\n    public function __construct(\n        public readonly string $id,\n        public readonly string $userId,\n        public readonly LLMConversation $conversation,\n        public readonly DateTime $createdAt,\n        public readonly DateTime $updatedAt,\n    ) {}\n\n    public function toArray(): array {\n        return [\n            'id' =&gt; $this-&gt;id,\n            'user_id' =&gt; $this-&gt;userId,\n            'conversation' =&gt; json_encode($this-&gt;conversation),\n            'created_at' =&gt; $this-&gt;createdAt-&gt;format('Y-m-d H:i:s'),\n            'updated_at' =&gt; $this-&gt;updatedAt-&gt;format('Y-m-d H:i:s'),\n        ];\n    }\n\n    public static function fromArray(array $data): self {\n        return new self(\n            id: $data['id'],\n            userId: $data['user_id'],\n            conversation: LLMConversation::fromJson(json_decode($data['conversation'], true)),\n            createdAt: new DateTime($data['created_at']),\n            updatedAt: new DateTime($data['updated_at']),\n        );\n    }\n}\n</code></pre>"},{"location":"examples/state-management/#multi-user-chat-application","title":"Multi-User Chat Application","text":"<pre><code>&lt;?php\nclass ChatService {\n    public function __construct(\n        private PDO $pdo,\n        private LLMChainClient $chainClient,\n        private $client,\n        private $model\n    ) {}\n\n    public function getOrCreateConversation(string $userId): LLMConversation {\n        $stmt = $this-&gt;pdo-&gt;prepare('\n            SELECT data FROM conversations\n            WHERE user_id = :user_id AND active = 1\n            ORDER BY updated_at DESC LIMIT 1\n        ');\n\n        $stmt-&gt;execute(['user_id' =&gt; $userId]);\n        $data = $stmt-&gt;fetchColumn();\n\n        if ($data) {\n            return LLMConversation::fromJson(json_decode($data, true));\n        }\n\n        // Create new conversation\n        $conversation = new LLMConversation();\n        $this-&gt;saveConversation($userId, $conversation);\n\n        return $conversation;\n    }\n\n    public function sendMessage(string $userId, string $message): string {\n        $conversation = $this-&gt;getOrCreateConversation($userId);\n\n        // Add user message (immutable - returns new instance)\n        $conversation = $conversation-&gt;withMessage(\n            LLMMessage::createFromUserString($message)\n        );\n\n        $response = $this-&gt;chainClient-&gt;run(\n            client: $this-&gt;client,\n            request: new LLMRequest(\n                model: $this-&gt;model,\n                conversation: $conversation,\n            )\n        );\n\n        // Add AI response (immutable - returns new instance)\n        $conversation = $conversation-&gt;withMessage($response-&gt;getLastMessage());\n        $this-&gt;saveConversation($userId, $conversation);\n\n        return $response-&gt;getLastText();\n    }\n\n    private function saveConversation(string $userId, LLMConversation $conversation): void {\n        $stmt = $this-&gt;pdo-&gt;prepare('\n            INSERT INTO conversations (user_id, data, updated_at)\n            VALUES (:user_id, :data, NOW())\n            ON DUPLICATE KEY UPDATE data = :data, updated_at = NOW()\n        ');\n\n        $stmt-&gt;execute([\n            'user_id' =&gt; $userId,\n            'data' =&gt; json_encode($conversation),\n        ]);\n    }\n}\n</code></pre>"},{"location":"examples/state-management/#conversation-exportimport","title":"Conversation Export/Import","text":""},{"location":"examples/state-management/#export","title":"Export","text":"<pre><code>&lt;?php\nfunction exportConversation(LLMConversation $conversation, string $filename): void {\n    $data = [\n        'version' =&gt; '1.0',\n        'exported_at' =&gt; date('c'),\n        'conversation' =&gt; json_decode(json_encode($conversation), true),\n    ];\n\n    file_put_contents($filename, json_encode($data, JSON_PRETTY_PRINT));\n}\n</code></pre>"},{"location":"examples/state-management/#import","title":"Import","text":"<pre><code>&lt;?php\nfunction importConversation(string $filename): LLMConversation {\n    $data = json_decode(file_get_contents($filename), true);\n\n    if ($data['version'] !== '1.0') {\n        throw new Exception('Unsupported conversation format');\n    }\n\n    // $data['conversation'] is already an array from json_decode\n    return LLMConversation::fromJson($data['conversation']);\n}\n</code></pre>"},{"location":"examples/state-management/#see-also","title":"See Also","text":"<ul> <li>Quick Start - Basic usage examples</li> <li>Configuration Guide - Configure conversations</li> </ul>"},{"location":"examples/tools-and-function-calling/","title":"Tools &amp; Function Calling","text":"<p>Enable AI agents to interact with external systems, databases, APIs, and custom code through tool/function calling.</p>"},{"location":"examples/tools-and-function-calling/#what-are-tools","title":"What are Tools?","text":"<p>Tools (also called function calling) allow LLMs to: - Read data: Fetch information from databases, APIs, files - Perform calculations: Execute complex computations - Take actions: Send emails, update databases, call webhooks - Access real-time data: Get current weather, stock prices, etc.</p> <p>The LLM decides when to use tools based on your prompts and the tool descriptions you provide.</p>"},{"location":"examples/tools-and-function-calling/#simple-tool-example","title":"Simple Tool Example","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Tool\\CallbackToolDefinition;\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\nuse Soukicz\\Llm\\Message\\LLMMessageText;\nuse Soukicz\\Llm\\LLMRequest;\n\n// Define a simple calculator tool\n$calculator = new CallbackToolDefinition(\n    name: 'calculator',\n    description: 'Perform basic arithmetic calculations',\n    inputSchema: [\n        'type' =&gt; 'object',\n        'properties' =&gt; [\n            'expression' =&gt; [\n                'type' =&gt; 'string',\n                'description' =&gt; 'Mathematical expression to evaluate (e.g., \"2 + 2\", \"10 * 5\")',\n            ],\n        ],\n        'required' =&gt; ['expression'],\n    ],\n    handler: function (array $input): LLMMessageContents {\n        $expression = $input['expression'];\n\n        // Safety: Basic eval protection (use a proper math parser in production!)\n        if (!preg_match('/^[\\d\\s+\\-*\\/().]+$/', $expression)) {\n            return LLMMessageContents::fromArrayData([\n                'error' =&gt; 'Invalid expression'\n            ]);\n        }\n\n        try {\n            $result = eval(\"return $expression;\");\n            return LLMMessageContents::fromArrayData([\n                'result' =&gt; $result,\n                'expression' =&gt; $expression\n            ]);\n        } catch (\\Throwable $e) {\n            return LLMMessageContents::fromArrayData([\n                'error' =&gt; $e-&gt;getMessage()\n            ]);\n        }\n    }\n);\n\n// Use the tool in a request\n$request = new LLMRequest(\n    model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n    conversation: new LLMConversation([\n        LLMMessage::createFromUserString('What is 157 * 832?')\n    ]),\n    tools: [$calculator]\n);\n\n$response = $chainClient-&gt;run($client, $request);\necho $response-&gt;getLastText(); // \"The result of 157 * 832 is 130,624.\"\n</code></pre>"},{"location":"examples/tools-and-function-calling/#database-query-tool","title":"Database Query Tool","text":"<pre><code>&lt;?php\nuse PDO;\n\n// Define a database query tool\nfunction createDatabaseTool(PDO $pdo): CallbackToolDefinition {\n    return new CallbackToolDefinition(\n        name: 'query_users',\n        description: 'Query the users database to find user information',\n        inputSchema: [\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'user_id' =&gt; [\n                    'type' =&gt; 'integer',\n                    'description' =&gt; 'The ID of the user to look up',\n                ],\n            ],\n            'required' =&gt; ['user_id'],\n        ],\n        handler: function (array $input) use ($pdo): LLMMessageContents {\n            $stmt = $pdo-&gt;prepare('SELECT * FROM users WHERE id = :id');\n            $stmt-&gt;execute(['id' =&gt; $input['user_id']]);\n            $user = $stmt-&gt;fetch(PDO::FETCH_ASSOC);\n\n            if (!$user) {\n                return LLMMessageContents::fromArrayData([\n                    'found' =&gt; false,\n                    'message' =&gt; 'User not found'\n                ]);\n            }\n\n            return LLMMessageContents::fromArrayData([\n                'found' =&gt; true,\n                'user' =&gt; [\n                    'id' =&gt; $user['id'],\n                    'name' =&gt; $user['name'],\n                    'email' =&gt; $user['email'],\n                    'created_at' =&gt; $user['created_at'],\n                ]\n            ]);\n        }\n    );\n}\n\n// Use in a customer support chatbot\n$request = new LLMRequest(\n    model: $model,\n    conversation: new LLMConversation([\n        LLMMessage::createFromUserString('Look up information for user ID 1234')\n    ]),\n    tools: [createDatabaseTool($pdo)]\n);\n</code></pre>"},{"location":"examples/tools-and-function-calling/#weather-api-tool","title":"Weather API Tool","text":"<pre><code>&lt;?php\n// Tool that calls an external API\n$weatherTool = new CallbackToolDefinition(\n    name: 'get_weather',\n    description: 'Get current weather for a city',\n    inputSchema: [\n        'type' =&gt; 'object',\n        'properties' =&gt; [\n            'city' =&gt; [\n                'type' =&gt; 'string',\n                'description' =&gt; 'City name (e.g., \"London\", \"New York\")',\n            ],\n            'units' =&gt; [\n                'type' =&gt; 'string',\n                'enum' =&gt; ['celsius', 'fahrenheit'],\n                'description' =&gt; 'Temperature units',\n            ],\n        ],\n        'required' =&gt; ['city'],\n    ],\n    handler: function (array $input): LLMMessageContents {\n        $city = $input['city'];\n        $units = $input['units'] ?? 'celsius';\n\n        // Call weather API (example using OpenWeatherMap)\n        $apiKey = getenv('OPENWEATHER_API_KEY');\n        $url = sprintf(\n            'https://api.openweathermap.org/data/2.5/weather?q=%s&amp;units=%s&amp;appid=%s',\n            urlencode($city),\n            $units === 'fahrenheit' ? 'imperial' : 'metric',\n            $apiKey\n        );\n\n        $response = file_get_contents($url);\n        $data = json_decode($response, true);\n\n        if (isset($data['cod']) &amp;&amp; $data['cod'] !== 200) {\n            return LLMMessageContents::fromArrayData([\n                'error' =&gt; 'City not found or API error'\n            ]);\n        }\n\n        return LLMMessageContents::fromArrayData([\n            'city' =&gt; $data['name'],\n            'temperature' =&gt; $data['main']['temp'],\n            'feels_like' =&gt; $data['main']['feels_like'],\n            'humidity' =&gt; $data['main']['humidity'],\n            'description' =&gt; $data['weather'][0]['description'],\n            'units' =&gt; $units,\n        ]);\n    }\n);\n</code></pre>"},{"location":"examples/tools-and-function-calling/#multiple-tools","title":"Multiple Tools","text":"<p>Provide multiple tools and let the LLM choose which to use:</p> <pre><code>&lt;?php\n$tools = [\n    // Weather tool\n    new CallbackToolDefinition(\n        name: 'get_weather',\n        description: 'Get current weather for a location',\n        inputSchema: [...],\n        handler: fn($input) =&gt; // weather logic\n    ),\n\n    // Stock price tool\n    new CallbackToolDefinition(\n        name: 'get_stock_price',\n        description: 'Get current stock price for a ticker symbol',\n        inputSchema: [\n            'type' =&gt; 'object',\n            'properties' =&gt; [\n                'ticker' =&gt; [\n                    'type' =&gt; 'string',\n                    'description' =&gt; 'Stock ticker symbol (e.g., \"AAPL\", \"GOOGL\")',\n                ],\n            ],\n            'required' =&gt; ['ticker'],\n        ],\n        handler: fn($input) =&gt; // stock API logic\n    ),\n\n    // Calculator\n    $calculator,\n];\n\n$request = new LLMRequest(\n    model: $model,\n    conversation: new LLMConversation([\n        LLMMessage::createFromUserString(\n            'What is the weather in London, and what is Apple stock price?'\n        )\n    ]),\n    tools: $tools\n);\n\n// The LLM will automatically call both tools and synthesize the results\n$response = $chainClient-&gt;run($client, $request);\n</code></pre>"},{"location":"examples/tools-and-function-calling/#multi-step-tool-usage","title":"Multi-Step Tool Usage","text":"<p>Handle conversations where the LLM uses tools multiple times:</p> <pre><code>&lt;?php\n$conversation = new LLMConversation([\n    LLMMessage::createFromUserString('Calculate 50 * 30, then add 100 to the result')\n]);\n\n// First request\n$response = $chainClient-&gt;run(\n    client: $client,\n    request: new LLMRequest(\n        model: $model,\n        conversation: $conversation,\n        tools: [$calculator]\n    )\n);\n\n// Check if the LLM used a tool\nif ($response-&gt;getLastMessage()-&gt;hasToolUse()) {\n    // Add the assistant's response (including tool use) to conversation\n    $conversation = $conversation-&gt;withMessage($response-&gt;getLastMessage());\n\n    // Execute the tool and add result\n    foreach ($response-&gt;getLastMessage()-&gt;getContents()-&gt;getToolUses() as $toolUse) {\n        $tool = $tools[$toolUse-&gt;getName()] ?? null;\n\n        if ($tool) {\n            $result = $tool-&gt;handle($toolUse-&gt;getInput());\n            $conversation = $conversation-&gt;withMessage(\n                LLMMessage::createFromUser(\n                    new LLMMessageContents([\n                        new LLMMessageToolResult(\n                            toolUseId: $toolUse-&gt;getId(),\n                            content: $result,\n                            isError: false\n                        )\n                    ])\n                )\n            );\n        }\n    }\n\n    // Continue the conversation\n    $finalResponse = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: $conversation,\n            tools: [$calculator]\n        )\n    );\n\n    echo $finalResponse-&gt;getLastText();\n}\n</code></pre>"},{"location":"examples/tools-and-function-calling/#tool-input-schema-best-practices","title":"Tool Input Schema Best Practices","text":"<p>The <code>inputSchema</code> follows JSON Schema format:</p> <pre><code>&lt;?php\n$inputSchema = [\n    'type' =&gt; 'object',\n    'properties' =&gt; [\n        'query' =&gt; [\n            'type' =&gt; 'string',\n            'description' =&gt; 'Clear description of what this parameter does',\n        ],\n        'limit' =&gt; [\n            'type' =&gt; 'integer',\n            'description' =&gt; 'Maximum number of results',\n            'minimum' =&gt; 1,\n            'maximum' =&gt; 100,\n        ],\n        'category' =&gt; [\n            'type' =&gt; 'string',\n            'enum' =&gt; ['tech', 'sports', 'politics'],\n            'description' =&gt; 'Category to filter by',\n        ],\n        'tags' =&gt; [\n            'type' =&gt; 'array',\n            'items' =&gt; ['type' =&gt; 'string'],\n            'description' =&gt; 'Array of tags',\n        ],\n    ],\n    'required' =&gt; ['query'], // Mandatory fields\n];\n</code></pre> <p>Best practices: - Provide clear, detailed descriptions for each property - Use <code>enum</code> for constrained choices - Set <code>minimum</code>/<code>maximum</code> for numbers - Mark required fields in the <code>required</code> array - Keep schemas simple - complex nested objects can confuse the model</p>"},{"location":"examples/tools-and-function-calling/#error-handling-in-tools","title":"Error Handling in Tools","text":"<p>Always handle errors gracefully:</p> <pre><code>&lt;?php\n$tool = new CallbackToolDefinition(\n    name: 'send_email',\n    description: 'Send an email',\n    inputSchema: [...],\n    handler: function (array $input): LLMMessageContents {\n        try {\n            // Validate input\n            if (!filter_var($input['to'], FILTER_VALIDATE_EMAIL)) {\n                return LLMMessageContents::fromArrayData([\n                    'success' =&gt; false,\n                    'error' =&gt; 'Invalid email address'\n                ]);\n            }\n\n            // Send email\n            $sent = mail($input['to'], $input['subject'], $input['body']);\n\n            return LLMMessageContents::fromArrayData([\n                'success' =&gt; $sent,\n                'message' =&gt; $sent ? 'Email sent successfully' : 'Failed to send email'\n            ]);\n        } catch (\\Throwable $e) {\n            return LLMMessageContents::fromArrayData([\n                'success' =&gt; false,\n                'error' =&gt; $e-&gt;getMessage()\n            ]);\n        }\n    }\n);\n</code></pre>"},{"location":"examples/tools-and-function-calling/#security-considerations","title":"Security Considerations","text":"<p>Important security guidelines:</p> <ol> <li>Validate all inputs: Never trust tool inputs blindly</li> <li>Use allowlists: Restrict what tools can access</li> <li>Avoid eval(): Never use eval() with user input</li> <li>Rate limiting: Prevent abuse of expensive tools</li> <li>Audit logging: Log all tool executions</li> <li>Permissions: Check user permissions before executing</li> </ol> <pre><code>&lt;?php\n// Example: Secure database tool\nfunction createSecureDatabaseTool(PDO $pdo, string $userId): CallbackToolDefinition {\n    return new CallbackToolDefinition(\n        name: 'query_data',\n        description: 'Query your data',\n        inputSchema: [...],\n        handler: function (array $input) use ($pdo, $userId): LLMMessageContents {\n            // Only allow accessing user's own data\n            $stmt = $pdo-&gt;prepare('\n                SELECT * FROM data\n                WHERE user_id = :user_id AND id = :id\n            ');\n\n            $stmt-&gt;execute([\n                'user_id' =&gt; $userId,  // Security: Scope to current user\n                'id' =&gt; $input['id']\n            ]);\n\n            // Audit log\n            error_log(\"Tool executed by user $userId: query_data\");\n\n            // Return results\n            return LLMMessageContents::fromArrayData([\n                'results' =&gt; $stmt-&gt;fetchAll(PDO::FETCH_ASSOC)\n            ]);\n        }\n    );\n}\n</code></pre>"},{"location":"examples/tools-and-function-calling/#testing-tools","title":"Testing Tools","text":"<p>Test tools independently before using with LLMs:</p> <pre><code>&lt;?php\n// Unit test a tool\n$calculator = new CallbackToolDefinition(...);\n\n$result = $calculator-&gt;handle(['expression' =&gt; '2 + 2']);\nassert($result-&gt;toArray()['result'] === 4);\n\n$result = $calculator-&gt;handle(['expression' =&gt; 'invalid']);\nassert(isset($result-&gt;toArray()['error']));\n</code></pre>"},{"location":"examples/tools-and-function-calling/#see-also","title":"See Also","text":"<ul> <li>Quick Start - Basic usage examples</li> <li>State Management - Saving tool-using conversations</li> </ul>"},{"location":"guides/batch-processing/","title":"Batch Processing","text":"<p>Process high volumes of LLM requests efficiently using batch operations. Batch processing is ideal for offline workloads where immediate responses aren't required.</p>"},{"location":"guides/batch-processing/#overview","title":"Overview","text":"<p>Batch processing allows you to: - Submit multiple requests at once - Process them asynchronously - Retrieve results later - Save costs (often 50% cheaper than real-time) - Handle large-scale operations</p> <p>Note: Batch processing support varies by provider. Check provider-specific documentation.</p>"},{"location":"guides/batch-processing/#llmbatchclient-interface","title":"LLMBatchClient Interface","text":"<p>Clients implementing batch operations use the <code>LLMBatchClient</code> interface:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\LLMBatchClient;\n\ninterface LLMBatchClient {\n    public function createBatch(array $requests): string;\n    public function retrieveBatch(string $batchId): ?array;\n    public function getCode(): string;\n}\n</code></pre>"},{"location":"guides/batch-processing/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/batch-processing/#submit-batch","title":"Submit Batch","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAIClient;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPT5;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\nuse Soukicz\\Llm\\Message\\LLMMessage;\n\n/** @var LLMBatchClient $client */\n$client = new OpenAIClient('sk-xxxxx', 'org-xxxxx');\n\n// Prepare multiple requests\n$requests = [];\nfor ($i = 0; $i &lt; 1000; $i++) {\n    $requests[] = new LLMRequest(\n        model: new GPT5(GPT5::VERSION_2025_08_07),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString(\"Summarize document $i\")\n        ])\n    );\n}\n\n// Submit batch\n$batchId = $client-&gt;createBatch($requests);\necho \"Batch created: $batchId\\n\";\n</code></pre>"},{"location":"guides/batch-processing/#retrieve-batch","title":"Retrieve Batch","text":"<pre><code>&lt;?php\n// Retrieve batch information (returns null if not ready, array with status and results when complete)\n$batch = $client-&gt;retrieveBatch($batchId);\n\nif ($batch !== null) {\n    // Batch information available\n    // Check provider-specific documentation for exact response format\n    var_dump($batch);\n}\n</code></pre>"},{"location":"guides/batch-processing/#complete-example","title":"Complete Example","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAIClient;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPT5;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\nuse Soukicz\\Llm\\Message\\LLMMessage;\n\n$client = new OpenAIClient('sk-xxxxx', 'org-xxxxx');\n\n// Prepare batch of classification tasks\n$texts = [\n    'This product is amazing!',\n    'Terrible service, would not recommend.',\n    'It\\'s okay, nothing special.',\n    // ... 1000s more\n];\n\n$requests = array_map(\n    fn($text) =&gt; new LLMRequest(\n        model: new GPT5(GPT5::VERSION_2025_08_07),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString(\"Classify sentiment (positive/negative/neutral): $text\")\n        ])\n    ),\n    $texts\n);\n\n// Submit batch\n$batchId = $client-&gt;createBatch($requests);\n\n// Poll until complete\ndo {\n    sleep(60); // Wait 1 minute\n    $batch = $client-&gt;retrieveBatch($batchId);\n\n    if ($batch !== null) {\n        // Check provider-specific response format for status\n        echo \"Batch retrieved\\n\";\n        break;\n    }\n} while (true);\n\n// Process batch results\n// Note: Exact format depends on provider implementation\nvar_dump($batch);\n</code></pre>"},{"location":"guides/batch-processing/#async-polling","title":"Async Polling","text":"<p>Use async operations for efficient polling:</p> <pre><code>&lt;?php\nuse React\\EventLoop\\Loop;\n\n$batchId = $client-&gt;createBatch($requests);\n\n// Check every 60 seconds\nLoop::addPeriodicTimer(60, function () use ($client, $batchId, &amp;$timer) {\n    $batch = $client-&gt;retrieveBatch($batchId);\n\n    if ($batch !== null) {\n        // Batch is available, process results\n        processResults($batch);\n        Loop::cancelTimer($timer);\n    }\n});\n\nLoop::run();\n</code></pre>"},{"location":"guides/batch-processing/#use-cases","title":"Use Cases","text":""},{"location":"guides/batch-processing/#data-processing","title":"Data Processing","text":"<p>Process large datasets:</p> <pre><code>&lt;?php\n// Classify 100k customer reviews\n$reviews = loadReviews(); // 100,000 reviews\n\n$batches = array_chunk($reviews, 1000); // Batch size of 1000\n\nforeach ($batches as $batchReviews) {\n    $requests = array_map(\n        fn($review) =&gt; createClassificationRequest($review),\n        $batchReviews\n    );\n\n    $batchIds[] = $client-&gt;createBatch($requests);\n}\n\n// Wait for all batches to complete\nwaitForBatches($batchIds);\n</code></pre>"},{"location":"guides/batch-processing/#content-generation","title":"Content Generation","text":"<p>Generate content at scale:</p> <pre><code>&lt;?php\n// Generate product descriptions for 10k products\n$products = loadProducts();\n\n$requests = array_map(\n    fn($product) =&gt; new LLMRequest(\n        model: $model,\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString(\"Write a compelling product description for: {$product-&gt;name}\")\n        ])\n    ),\n    $products\n);\n\n$batchId = $client-&gt;createBatch($requests);\n</code></pre>"},{"location":"guides/batch-processing/#translation","title":"Translation","text":"<p>Batch translate documents:</p> <pre><code>&lt;?php\n// Translate 1000 documents to 5 languages\n$documents = loadDocuments();\n$languages = ['es', 'fr', 'de', 'it', 'pt'];\n\n$requests = [];\nforeach ($documents as $doc) {\n    foreach ($languages as $lang) {\n        $requests[] = new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUserString(\"Translate to $lang: {$doc-&gt;content}\")\n            ])\n        );\n    }\n}\n\n$batchId = $client-&gt;createBatch($requests);\n</code></pre>"},{"location":"guides/batch-processing/#best-practices","title":"Best Practices","text":"<ol> <li>Batch sizing - Keep batches at 1000-10000 requests for optimal processing</li> <li>Polling interval - Poll every 60-300 seconds, not more frequently</li> <li>Error handling - Handle failed batches gracefully</li> <li>Cost monitoring - Track batch costs across operations</li> <li>Result storage - Save results immediately after retrieval</li> <li>Timeout handling - Set reasonable timeouts for batch completion</li> <li>Rate limits - Respect provider rate limits on batch creation</li> </ol>"},{"location":"guides/batch-processing/#error-handling","title":"Error Handling","text":"<pre><code>&lt;?php\ntry {\n    $batchId = $client-&gt;createBatch($requests);\n} catch (BatchCreationException $e) {\n    // Handle batch creation error\n    echo \"Failed to create batch: \" . $e-&gt;getMessage();\n\n    // Retry with smaller batch size\n    $smallerBatches = array_chunk($requests, 500);\n    foreach ($smallerBatches as $batch) {\n        $batchId = $client-&gt;createBatch($batch);\n    }\n}\n\n// Retrieve batch results\n$batch = $client-&gt;retrieveBatch($batchId);\nif ($batch !== null) {\n    // Process batch results according to provider-specific format\n    // Check provider documentation for exact structure\n    processResults($batch);\n}\n</code></pre>"},{"location":"guides/batch-processing/#cost-comparison","title":"Cost Comparison","text":"<p>Batch processing typically offers 50% cost savings:</p> <pre><code>&lt;?php\n// Real-time: $0.01 per request \u00d7 10,000 = $100\n$realTimeCost = 10000 * 0.01;\n\n// Batch: $0.005 per request \u00d7 10,000 = $50\n$batchCost = 10000 * 0.005;\n\necho \"Savings: $\" . ($realTimeCost - $batchCost); // $50\n</code></pre>"},{"location":"guides/batch-processing/#provider-support","title":"Provider Support","text":"<ul> <li>\u2705 OpenAI - Full batch API support</li> <li>\u26a0\ufe0f Anthropic - Check current API documentation</li> <li>\u26a0\ufe0f Google Gemini - Check current API documentation</li> <li>\u274c OpenAI-compatible - Varies by provider</li> </ul>"},{"location":"guides/batch-processing/#limitations","title":"Limitations","text":"<ul> <li>Latency - Results may take minutes to hours</li> <li>No streaming - Batch responses don't support streaming</li> <li>No cancellation - Some providers don't allow batch cancellation</li> <li>Result expiration - Results may expire after 24-48 hours</li> <li>Size limits - Maximum batch size varies by provider</li> </ul>"},{"location":"guides/batch-processing/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - Request configuration</li> <li>Provider Documentation - Provider-specific batch features</li> </ul>"},{"location":"guides/caching/","title":"Caching","text":"<p>Reduce costs and latency with intelligent HTTP-level response caching. PHP LLM caches LLM responses automatically, making repeated requests nearly instant and free.</p>"},{"location":"guides/caching/#overview","title":"Overview","text":"<p>All PHP LLM clients support caching at the HTTP request level. When enabled: - Identical requests return cached responses instantly - No API calls are made for cached requests - Original response time is preserved in metadata - Costs are eliminated for cached responses</p>"},{"location":"guides/caching/#file-cache","title":"File Cache","text":"<p>The built-in <code>FileCache</code> stores responses on the filesystem:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\n\n$cache = new FileCache(sys_get_temp_dir());\n$client = new AnthropicClient('sk-xxxxx', $cache);\n</code></pre> <p>Characteristics: - \u2705 Simple to set up - \u2705 No additional dependencies - \u2705 Works across requests - \u26a0\ufe0f Limited to single server - \u26a0\ufe0f Manual cleanup required</p>"},{"location":"guides/caching/#custom-cache-directory","title":"Custom Cache Directory","text":"<pre><code>&lt;?php\n$cache = new FileCache('/var/cache/llm');\n$client = new AnthropicClient('sk-xxxxx', $cache);\n</code></pre>"},{"location":"guides/caching/#dynamodb-cache","title":"DynamoDB Cache","text":"<p>For distributed systems, use the DynamoDB cache extension:</p> <pre><code>composer require soukicz/llm-cache-dynamodb\n</code></pre> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\DynamoDB\\DynamoDBCache;\nuse Aws\\DynamoDb\\DynamoDbClient;\n\n$dynamodb = new DynamoDbClient([\n    'region' =&gt; 'us-east-1',\n    'version' =&gt; 'latest',\n]);\n\n$cache = new DynamoDBCache($dynamodb, 'llm-cache-table');\n$client = new AnthropicClient('sk-xxxxx', $cache);\n</code></pre> <p>Characteristics: - \u2705 Distributed across servers - \u2705 Automatic TTL expiration - \u2705 Scalable - \u274c Requires AWS setup - \u274c Additional costs</p>"},{"location":"guides/caching/#custom-cache-implementation","title":"Custom Cache Implementation","text":"<p>Implement the <code>CacheInterface</code> for custom caching:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\CacheInterface;\n\nclass RedisCache implements CacheInterface {\n    public function __construct(\n        private Redis $redis,\n        private int $ttl = 3600\n    ) {}\n\n    public function get(string $key): ?string {\n        $value = $this-&gt;redis-&gt;get($key);\n        return $value !== false ? $value : null;\n    }\n\n    public function set(string $key, string $value): void {\n        $this-&gt;redis-&gt;setex($key, $this-&gt;ttl, $value);\n    }\n\n    public function has(string $key): bool {\n        return $this-&gt;redis-&gt;exists($key) &gt; 0;\n    }\n\n    public function delete(string $key): void {\n        $this-&gt;redis-&gt;del($key);\n    }\n}\n</code></pre> <pre><code>&lt;?php\n$cache = new RedisCache($redisClient);\n$client = new AnthropicClient('sk-xxxxx', $cache);\n</code></pre>"},{"location":"guides/caching/#cache-keys","title":"Cache Keys","text":"<p>Cache keys are generated from: - API endpoint - Model name and version - Request parameters (temperature, maxTokens, etc.) - Conversation messages - Tool definitions</p> <p>Important: Always use exact model versions to prevent stale cached responses.</p>"},{"location":"guides/caching/#best-practices","title":"Best Practices","text":""},{"location":"guides/caching/#use-exact-model-versions","title":"Use Exact Model Versions","text":"<p>\u274c Bad - Generic naming <pre><code>&lt;?php\n// Vague version could cache responses from old models\n$model = new AnthropicClaude45Sonnet('latest');\n</code></pre></p> <p>\u2705 Good - Explicit version <pre><code>&lt;?php\n// Specific version ensures cache correctness\n$model = new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929);\n</code></pre></p>"},{"location":"guides/caching/#development-vs-production","title":"Development vs Production","text":"<p>Development: <pre><code>&lt;?php\n// Aggressive caching to save costs during development\n$cache = new FileCache('/tmp/llm-cache');\n$client = new AnthropicClient('sk-xxxxx', $cache);\n</code></pre></p> <p>Production: <pre><code>&lt;?php\n// Distributed cache for multi-server setup\n$cache = new DynamoDBCache($dynamodb, 'prod-llm-cache');\n$client = new AnthropicClient('sk-xxxxx', $cache);\n</code></pre></p>"},{"location":"guides/caching/#cache-warming","title":"Cache Warming","text":"<p>Pre-cache common requests:</p> <pre><code>&lt;?php\n// Warm cache with common queries\n$commonQueries = [\n    'What is PHP?',\n    'How do I install composer?',\n    'What are PHP traits?',\n];\n\nforeach ($commonQueries as $query) {\n    $response = $chainClient-&gt;run(\n        client: $client,\n        request: new LLMRequest(\n            model: $model,\n            conversation: new LLMConversation([\n                LLMMessage::createFromUserString($query)\n            ])\n        )\n    );\n}\n</code></pre>"},{"location":"guides/caching/#disabling-cache","title":"Disabling Cache","text":"<p>To bypass cache for specific requests, create a client without cache:</p> <pre><code>&lt;?php\n// No cache\n$client = new AnthropicClient('sk-xxxxx', null);\n</code></pre>"},{"location":"guides/caching/#cache-behavior","title":"Cache Behavior","text":""},{"location":"guides/caching/#what-gets-cached","title":"What Gets Cached","text":"<p>\u2705 Successful responses \u2705 Complete conversations \u2705 Tool call results \u2705 Multimodal requests</p>"},{"location":"guides/caching/#what-doesnt-get-cached","title":"What Doesn't Get Cached","text":"<p>\u274c Failed requests (errors) \u274c Incomplete responses \u274c Stream responses (partial) \u274c Async requests in progress</p>"},{"location":"guides/caching/#monitoring-cache-performance","title":"Monitoring Cache Performance","text":"<p>Track cache hit rates:</p> <pre><code>&lt;?php\nclass CacheMonitor implements CacheInterface {\n    private int $hits = 0;\n    private int $misses = 0;\n\n    public function __construct(\n        private CacheInterface $cache\n    ) {}\n\n    public function get(string $key): ?string {\n        $value = $this-&gt;cache-&gt;get($key);\n        if ($value !== null) {\n            $this-&gt;hits++;\n        } else {\n            $this-&gt;misses++;\n        }\n        return $value;\n    }\n\n    public function set(string $key, string $value): void {\n        $this-&gt;cache-&gt;set($key, $value);\n    }\n\n    public function getHitRate(): float {\n        $total = $this-&gt;hits + $this-&gt;misses;\n        return $total &gt; 0 ? $this-&gt;hits / $total : 0;\n    }\n\n    // Implement other interface methods...\n}\n</code></pre> <pre><code>&lt;?php\n$cache = new CacheMonitor(new FileCache('/tmp/cache'));\n$client = new AnthropicClient('sk-xxxxx', $cache);\n\n// After some requests...\necho \"Cache hit rate: \" . ($cache-&gt;getHitRate() * 100) . \"%\\n\";\n</code></pre>"},{"location":"guides/caching/#cache-expiration","title":"Cache Expiration","text":""},{"location":"guides/caching/#manual-cleanup","title":"Manual Cleanup","text":"<pre><code>&lt;?php\n// Clear specific cache entry\n$cache-&gt;delete($cacheKey);\n\n// Clear all cache (FileCache example)\narray_map('unlink', glob('/tmp/llm-cache/*'));\n</code></pre>"},{"location":"guides/caching/#automatic-expiration","title":"Automatic Expiration","text":"<p>Implement TTL in custom cache:</p> <pre><code>&lt;?php\nclass TTLFileCache implements CacheInterface {\n    private int $ttl;\n\n    public function __construct(string $directory, int $ttlSeconds = 3600) {\n        $this-&gt;directory = $directory;\n        $this-&gt;ttl = $ttlSeconds;\n    }\n\n    public function get(string $key): ?string {\n        $file = $this-&gt;getFilePath($key);\n\n        if (!file_exists($file)) {\n            return null;\n        }\n\n        // Check if expired\n        if (time() - filemtime($file) &gt; $this-&gt;ttl) {\n            unlink($file);\n            return null;\n        }\n\n        return file_get_contents($file);\n    }\n\n    // Implement other methods...\n}\n</code></pre>"},{"location":"guides/caching/#cost-savings","title":"Cost Savings","text":"<p>Example cost calculation:</p> <pre><code>&lt;?php\n$request = new LLMRequest(/*...*/);\n\n// First request - hits API ($0.015)\n$response1 = $chainClient-&gt;run($client, $request);\necho \"Cost: $\" . $response1-&gt;getTokenUsage()-&gt;getTotalCost() . \"\\n\";\n\n// Cached request - no cost ($0.00)\n$response2 = $chainClient-&gt;run($client, $request);\necho \"Cost: $\" . $response2-&gt;getTokenUsage()-&gt;getTotalCost() . \"\\n\";\n\n// 100% savings on repeated requests!\n</code></pre>"},{"location":"guides/caching/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - Client configuration</li> <li>Examples - Cache usage examples</li> </ul>"},{"location":"guides/configuration/","title":"Configuration Options","text":"<p>Configure your AI agent requests with various parameters to control behavior, output, and resource usage.</p>"},{"location":"guides/configuration/#llmrequest-parameters","title":"LLMRequest Parameters","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Config\\ReasoningBudget;\nuse Soukicz\\Llm\\Config\\ReasoningEffort;\nuse Soukicz\\Llm\\LLMRequest;\n\n$request = new LLMRequest(\n    model: $model,                              // Required: Model instance\n    conversation: $conversation,                // Required: LLMConversation\n    tools: $tools,                              // Optional: Array of tool definitions\n    temperature: 0.7,                           // Optional: 0.0 to 1.0\n    maxTokens: 4096,                            // Optional: Maximum response tokens\n    stopSequences: ['###', 'END'],              // Optional: Stop generation strings\n    reasoningConfig: ReasoningEffort::HIGH,     // Optional: For reasoning models\n    reasoningConfig: new ReasoningBudget(10000),// Optional: Token budget for reasoning\n);\n</code></pre>"},{"location":"guides/configuration/#core-parameters","title":"Core Parameters","text":""},{"location":"guides/configuration/#model-required","title":"model (Required)","text":"<p>The LLM model to use. Create model instances from provider-specific classes:</p> <pre><code>&lt;?php\n// Anthropic\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\n$model = new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929);\n\n// OpenAI\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPT5;\n$model = new GPT5(GPT5::VERSION_2025_08_07);\n\n// Gemini\nuse Soukicz\\Llm\\Client\\Gemini\\Model\\Gemini25Pro;\n$model = new Gemini25Pro();\n</code></pre>"},{"location":"guides/configuration/#conversation-required","title":"conversation (Required)","text":"<p>An <code>LLMConversation</code> containing message history:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\Message\\LLMMessage;\n\n$conversation = new LLMConversation([\n    LLMMessage::createFromUserString('Hello'),\n    LLMMessage::createFromAssistantString('Hi! How can I help?'),\n    LLMMessage::createFromUserString('What is PHP?'),\n]);\n</code></pre>"},{"location":"guides/configuration/#optional-parameters","title":"Optional Parameters","text":""},{"location":"guides/configuration/#temperature","title":"temperature","text":"<p>Controls randomness in responses (0.0 to 1.0):</p> <ul> <li>0.0 - Deterministic, focused responses</li> <li>0.5 - Balanced (default for most models)</li> <li>1.0 - Creative, varied responses</li> </ul> <pre><code>&lt;?php\n// Precise, factual responses\n$request = new LLMRequest(\n    model: $model,\n    conversation: $conversation,\n    temperature: 0.0\n);\n\n// Creative writing\n$request = new LLMRequest(\n    model: $model,\n    conversation: $conversation,\n    temperature: 0.9\n);\n</code></pre>"},{"location":"guides/configuration/#maxtokens","title":"maxTokens","text":"<p>Maximum number of tokens in the response:</p> <pre><code>&lt;?php\n$request = new LLMRequest(\n    model: $model,\n    conversation: $conversation,\n    maxTokens: 1000  // Limit response to ~750 words\n);\n</code></pre> <p>Note: Different providers have different maximum limits. Check provider documentation.</p>"},{"location":"guides/configuration/#stopsequences","title":"stopSequences","text":"<p>Array of strings that stop generation when encountered:</p> <pre><code>&lt;?php\n$request = new LLMRequest(\n    model: $model,\n    conversation: $conversation,\n    stopSequences: ['###', 'END', '\\n\\n---']\n);\n</code></pre> <p>Useful for: - Structured output formats - Limiting response sections - Custom delimiters</p>"},{"location":"guides/configuration/#tools","title":"tools","text":"<p>Array of tool definitions for function calling:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Tool\\CallbackToolDefinition;\n\n$tools = [\n    new CallbackToolDefinition(\n        name: 'search',\n        description: 'Search the web',\n        inputSchema: ['type' =&gt; 'object', 'properties' =&gt; ['query' =&gt; ['type' =&gt; 'string']]],\n        handler: fn($input) =&gt; searchWeb($input['query'])\n    ),\n];\n\n$request = new LLMRequest(\n    model: $model,\n    conversation: $conversation,\n    tools: $tools\n);\n</code></pre> <p>See Tools Guide for detailed tool documentation.</p>"},{"location":"guides/configuration/#reasoning-parameters","title":"Reasoning Parameters","text":"<p>For reasoning models (o3, o4):</p>"},{"location":"guides/configuration/#reasoningeffort","title":"reasoningEffort","text":"<p>Control computational effort:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Config\\ReasoningEffort;\n\n$request = new LLMRequest(\n    model: new GPTo3(GPTo3::VERSION_2025_04_16),\n    conversation: $conversation,\n    reasoningConfig: ReasoningEffort::HIGH\n);\n</code></pre> <p>Options: - <code>ReasoningEffort::LOW</code> - Fast, less thorough - <code>ReasoningEffort::MEDIUM</code> - Balanced (default) - <code>ReasoningEffort::HIGH</code> - Thorough, slower</p>"},{"location":"guides/configuration/#reasoningconfig","title":"reasoningConfig","text":"<p>Limit reasoning tokens for cost control using <code>ReasoningBudget</code>:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Config\\ReasoningBudget;\n\n$request = new LLMRequest(\n    model: new GPTo3(GPTo3::VERSION_2025_04_16),\n    conversation: $conversation,\n    reasoningConfig: new ReasoningBudget(5000)  // Max 5k reasoning tokens\n);\n</code></pre> <p>See Reasoning Models Guide for more details.</p>"},{"location":"guides/configuration/#client-configuration","title":"Client Configuration","text":""},{"location":"guides/configuration/#cache-configuration","title":"Cache Configuration","text":"<p>Configure caching when creating clients:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\n\n$cache = new FileCache(sys_get_temp_dir());\n$client = new AnthropicClient('sk-xxxxx', $cache);\n</code></pre> <p>See Caching Guide for cache options.</p>"},{"location":"guides/configuration/#http-middleware","title":"HTTP Middleware","text":"<p>Add Guzzle middleware for logging or custom behavior:</p> <pre><code>&lt;?php\nuse GuzzleHttp\\HandlerStack;\nuse GuzzleHttp\\Middleware;\n\n$stack = HandlerStack::create();\n$stack-&gt;push(Middleware::log($logger, $formatter));\n\n$client = new AnthropicClient(\n    apiKey: 'sk-xxxxx',\n    cache: $cache,\n    handler: $stack\n);\n</code></pre>"},{"location":"guides/configuration/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"guides/configuration/#gemini-safety-settings","title":"Gemini Safety Settings","text":"<p>Configure content safety filters using the Gemini API safety settings format:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\Gemini\\GeminiClient;\n\n// Permissive - Allow most content\n$permissiveSafetySettings = [\n    ['category' =&gt; 'HARM_CATEGORY_HARASSMENT', 'threshold' =&gt; 'BLOCK_NONE'],\n    ['category' =&gt; 'HARM_CATEGORY_HATE_SPEECH', 'threshold' =&gt; 'BLOCK_NONE'],\n    ['category' =&gt; 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold' =&gt; 'BLOCK_NONE'],\n    ['category' =&gt; 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold' =&gt; 'BLOCK_NONE'],\n];\n\n$client = new GeminiClient(\n    apiKey: 'your-api-key',\n    safetySettings: $permissiveSafetySettings\n);\n\n// Strict - Block harmful content\n$strictSafetySettings = [\n    ['category' =&gt; 'HARM_CATEGORY_HARASSMENT', 'threshold' =&gt; 'BLOCK_LOW_AND_ABOVE'],\n    ['category' =&gt; 'HARM_CATEGORY_HATE_SPEECH', 'threshold' =&gt; 'BLOCK_LOW_AND_ABOVE'],\n    ['category' =&gt; 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold' =&gt; 'BLOCK_LOW_AND_ABOVE'],\n    ['category' =&gt; 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold' =&gt; 'BLOCK_LOW_AND_ABOVE'],\n];\n\n$client = new GeminiClient(\n    apiKey: 'your-api-key',\n    safetySettings: $strictSafetySettings\n);\n</code></pre>"},{"location":"guides/configuration/#openai-compatible-clients","title":"OpenAI-Compatible Clients","text":"<p>Custom base URL and model names:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAICompatibleClient;\nuse Soukicz\\Llm\\Client\\Universal\\LocalModel;\n\n$client = new OpenAICompatibleClient(\n    apiKey: 'your-api-key',\n    baseUrl: 'https://api.openrouter.ai/v1'\n);\n\n$model = new LocalModel('anthropic/claude-3.5-sonnet');\n</code></pre>"},{"location":"guides/configuration/#configuration-best-practices","title":"Configuration Best Practices","text":"<ol> <li>Use environment variables for API keys</li> <li>Enable caching for development to save costs</li> <li>Set reasonable maxTokens to prevent runaway costs</li> <li>Use lower temperature for factual tasks</li> <li>Use higher temperature for creative tasks</li> <li>Set stopSequences for structured outputs</li> <li>Configure safety settings appropriately for your use case</li> <li>Use reasoning budgets in production</li> </ol>"},{"location":"guides/configuration/#example-complete-configuration","title":"Example: Complete Configuration","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\nuse Soukicz\\Llm\\Client\\LLMChainClient;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\nuse Soukicz\\Llm\\Message\\LLMMessage;\n\n// Configure client with cache\n$cache = new FileCache(sys_get_temp_dir());\n$client = new AnthropicClient(\n    apiKey: getenv('ANTHROPIC_API_KEY'),\n    cache: $cache\n);\n\n$chainClient = new LLMChainClient();\n\n// Configure request\n$request = new LLMRequest(\n    model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n    conversation: new LLMConversation([\n        LLMMessage::createFromUserString('Write a short poem about PHP')\n    ]),\n    temperature: 0.8,          // Creative\n    maxTokens: 500,            // ~375 words max\n    stopSequences: ['---'],    // Stop at delimiter\n);\n\n$response = $chainClient-&gt;run($client, $request);\n</code></pre>"},{"location":"guides/configuration/#see-also","title":"See Also","text":"<ul> <li>Reasoning Models - Reasoning-specific configuration</li> <li>Tools Guide - Tool configuration</li> <li>Caching Guide - Cache configuration</li> <li>Provider Documentation - Provider-specific options</li> </ul>"},{"location":"guides/feedback-loops/","title":"Feedback Loops","text":"<p>Build self-correcting AI agents with feedback loops. The <code>LLMChainClient</code> allows you to validate agent responses and automatically request improvements until quality criteria are met. This is essential for building reliable agentic systems that produce consistent, validated outputs.</p>"},{"location":"guides/feedback-loops/#overview","title":"Overview","text":"<p>Feedback loops enable your AI agents to: 1. Generate a response 2. Validate the output against your criteria 3. Request corrections if needed 4. Iterate until the response meets requirements</p> <p>This creates agents that can self-correct and improve their outputs without manual intervention.</p>"},{"location":"guides/feedback-loops/#basic-feedback-loop","title":"Basic Feedback Loop","text":"<p>Define a callback function to validate responses:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\nuse Soukicz\\Llm\\Client\\LLMChainClient;\nuse Soukicz\\Llm\\LLMResponse;\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\n\nrequire_once __DIR__ . '/vendor/autoload.php';\n\n$cache = new FileCache(sys_get_temp_dir());\n$anthropic = new AnthropicClient('sk-xxxxxx', $cache);\n$chainClient = new LLMChainClient();\n\n$response = $chainClient-&gt;run(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString('List 5 animals in JSON array and wrap this array in XML tag named \"animals\"')\n        ]),\n    ),\n    feedbackCallback: function (LLMResponse $llmResponse): ?LLMMessage {\n        // Validate the response\n        if (preg_match('~&lt;animals&gt;(.+)&lt;/animals&gt;~s', $llmResponse-&gt;getLastText(), $m)) {\n            try {\n                json_decode($m[1], true, 512, JSON_THROW_ON_ERROR);\n                return null; // Valid response - stop iteration\n            } catch (JsonException $e) {\n                // Invalid JSON - request correction\n                return LLMMessage::createFromUserString(\n                    'I am sorry, but the response is not a valid JSON (' . $e-&gt;getMessage() . '). Please respond again.'\n                );\n            }\n        }\n\n        // Missing XML tag - request correction\n        return LLMMessage::createFromUserString(\n            'I am sorry, but I could not find animals tag in the response. Please respond again.'\n        );\n    }\n);\n\necho $response-&gt;getLastText();\n</code></pre>"},{"location":"guides/feedback-loops/#feedback-callback-return-values","title":"Feedback Callback Return Values","text":"<p>The feedback callback should return:</p> <ul> <li><code>null</code> - Response is valid, stop iteration</li> <li><code>LLMMessage</code> - Response needs improvement, send this message back to the agent</li> </ul>"},{"location":"guides/feedback-loops/#nested-llm-validation","title":"Nested LLM Validation","text":"<p>Use another LLM to validate complex responses:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude35Haiku;\n\n$response = $chainClient-&gt;run(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString('List all US states in JSON array and wrap this array in XML tag named \"states\"')\n        ]),\n    ),\n    feedbackCallback: function (LLMResponse $llmResponse) use ($anthropic, $chainClient): ?LLMMessage {\n        if (preg_match('~&lt;/states&gt;(.+)~s', $llmResponse-&gt;getLastText(), $m)) {\n            $suffix = trim(trim(trim($m[1]), '`'));\n            if (empty($suffix)) {\n                return null; // Complete\n            }\n\n            // Use a cheaper, faster model to validate\n            $checkResponse = $chainClient-&gt;run(\n                client: $anthropic,\n                request: new LLMRequest(\n                    model: new AnthropicClaude35Haiku(AnthropicClaude35Haiku::VERSION_20241022),\n                    conversation: new LLMConversation([\n                        LLMMessage::createFromUserString(&lt;&lt;&lt;EOT\nI need help with understanding of text. I have submitted work and I have received following text at the end of response:\n\n&lt;response-text&gt;\n$suffix\n&lt;/response-text&gt;\n\nI need you to decide if this means that work was completed or if I should request continuation of work. Briefly explain what you see in response and finally output WORK_COMPLETED or WORK_NOT_COMPLETED. This is automated process and I need one of these two outputs.\nEOT\n                        ),\n                    ]),\n                )\n            );\n\n            if (str_contains($checkResponse-&gt;getLastText(), 'WORK_COMPLETED')) {\n                return null; // Validated as complete\n            }\n\n            return LLMMessage::createFromUserString('Please continue');\n        }\n\n        return null;\n    }\n);\n\necho $response-&gt;getLastText();\n</code></pre>"},{"location":"guides/feedback-loops/#common-validation-patterns","title":"Common Validation Patterns","text":""},{"location":"guides/feedback-loops/#format-validation","title":"Format Validation","text":"<pre><code>&lt;?php\nfeedbackCallback: function (LLMResponse $response): ?LLMMessage {\n    $text = $response-&gt;getLastText();\n\n    // Check for JSON format\n    if (!json_decode($text)) {\n        return LLMMessage::createFromUserString('Please provide valid JSON');\n    }\n\n    return null;\n}\n</code></pre>"},{"location":"guides/feedback-loops/#content-requirements","title":"Content Requirements","text":"<pre><code>&lt;?php\nfeedbackCallback: function (LLMResponse $response): ?LLMMessage {\n    $text = $response-&gt;getLastText();\n\n    // Ensure response contains required information\n    if (!str_contains($text, 'conclusion')) {\n        return LLMMessage::createFromUserString('Please include a conclusion section');\n    }\n\n    return null;\n}\n</code></pre>"},{"location":"guides/feedback-loops/#length-constraints","title":"Length Constraints","text":"<pre><code>&lt;?php\nfeedbackCallback: function (LLMResponse $response): ?LLMMessage {\n    $text = $response-&gt;getLastText();\n    $wordCount = str_word_count($text);\n\n    if ($wordCount &lt; 100) {\n        return LLMMessage::createFromUserString('Please provide a more detailed response (at least 100 words)');\n    }\n\n    if ($wordCount &gt; 500) {\n        return LLMMessage::createFromUserString('Please make the response more concise (max 500 words)');\n    }\n\n    return null;\n}\n</code></pre>"},{"location":"guides/feedback-loops/#schema-validation","title":"Schema Validation","text":"<pre><code>&lt;?php\nfeedbackCallback: function (LLMResponse $response): ?LLMMessage {\n    $data = json_decode($response-&gt;getLastText(), true);\n\n    if (!isset($data['name']) || !isset($data['email'])) {\n        return LLMMessage::createFromUserString('Response must include name and email fields');\n    }\n\n    if (!filter_var($data['email'], FILTER_VALIDATE_EMAIL)) {\n        return LLMMessage::createFromUserString('Please provide a valid email address');\n    }\n\n    return null;\n}\n</code></pre>"},{"location":"guides/feedback-loops/#preventing-infinite-loops","title":"Preventing Infinite Loops","text":"<p>Always implement safeguards to prevent infinite loops:</p>"},{"location":"guides/feedback-loops/#iteration-counter","title":"Iteration Counter","text":"<pre><code>&lt;?php\n$maxIterations = 5;\n$iteration = 0;\n\nfeedbackCallback: function (LLMResponse $response) use (&amp;$iteration, $maxIterations): ?LLMMessage {\n    $iteration++;\n\n    if ($iteration &gt;= $maxIterations) {\n        // Stop after max attempts\n        return null;\n    }\n\n    // Your validation logic\n    if (!isValid($response)) {\n        return LLMMessage::createFromUserString('Please try again');\n    }\n\n    return null;\n}\n</code></pre>"},{"location":"guides/feedback-loops/#progressive-feedback","title":"Progressive Feedback","text":"<p>Provide more specific guidance with each iteration:</p> <pre><code>&lt;?php\n$attempt = 0;\n\nfeedbackCallback: function (LLMResponse $response) use (&amp;$attempt): ?LLMMessage {\n    $attempt++;\n\n    if (!isValid($response)) {\n        if ($attempt === 1) {\n            return LLMMessage::createFromUserString('The format is incorrect');\n        } elseif ($attempt === 2) {\n            return LLMMessage::createFromUserString('Remember to use JSON format with \"name\" and \"age\" fields');\n        } else {\n            return LLMMessage::createFromUserString('Example: {\"name\": \"John\", \"age\": 30}');\n        }\n    }\n\n    return null;\n}\n</code></pre>"},{"location":"guides/feedback-loops/#combining-with-other-features","title":"Combining with Other Features","text":""},{"location":"guides/feedback-loops/#with-tools","title":"With Tools","text":"<p>Validate tool outputs in feedback loops:</p> <pre><code>&lt;?php\n$response = $chainClient-&gt;run(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: $conversation,\n        tools: [$calculatorTool],\n    ),\n    feedbackCallback: function (LLMResponse $response): ?LLMMessage {\n        // Ensure the agent used the calculator tool\n        if (!$response-&gt;hasToolCalls()) {\n            return LLMMessage::createFromUserString('Please use the calculator tool for this calculation');\n        }\n        return null;\n    }\n);\n</code></pre>"},{"location":"guides/feedback-loops/#with-reasoning-models","title":"With Reasoning Models","text":"<p>Validate reasoning model outputs:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\OpenAIGPTo3;\nuse Soukicz\\Llm\\Config\\ReasoningEffort;\n\n$response = $chainClient-&gt;run(\n    client: $openai,\n    request: new LLMRequest(\n        model: new OpenAIGPTo3(),\n        conversation: $conversation,\n        reasoningEffort: ReasoningEffort::HIGH\n    ),\n    feedbackCallback: function (LLMResponse $response): ?LLMMessage {\n        // Verify mathematical accuracy\n        if (!verifyCalculation($response-&gt;getLastText())) {\n            return LLMMessage::createFromUserString('The calculation appears incorrect. Please verify your work.');\n        }\n        return null;\n    }\n);\n</code></pre>"},{"location":"guides/feedback-loops/#best-practices","title":"Best Practices","text":"<ol> <li>Always set iteration limits - Prevent infinite loops</li> <li>Provide specific feedback - Tell the agent exactly what's wrong</li> <li>Use cheaper models for validation - Save costs by using fast models for checks</li> <li>Log validation failures - Track when and why validation fails</li> <li>Progressive guidance - Provide more detail with each failed attempt</li> <li>Early termination - Return <code>null</code> as soon as criteria are met</li> <li>Validate incrementally - Check simple criteria first, complex ones later</li> </ol>"},{"location":"guides/feedback-loops/#common-pitfalls","title":"Common Pitfalls","text":"<p>\u274c No iteration limit <pre><code>&lt;?php\n// BAD: Could loop forever\nfeedbackCallback: function ($response) {\n    return !isValid($response) ? LLMMessage::createFromUserString('Try again') : null;\n}\n</code></pre></p> <p>\u2705 With iteration limit <pre><code>&lt;?php\n// GOOD: Maximum attempts enforced\n$attempts = 0;\nfeedbackCallback: function ($response) use (&amp;$attempts) {\n    $attempts++;\n    if ($attempts &gt;= 5) return null;\n    return !isValid($response) ? LLMMessage::createFromUserString('Try again') : null;\n}\n</code></pre></p> <p>\u274c Vague feedback <pre><code>&lt;?php\n// BAD: Agent doesn't know what's wrong\nreturn LLMMessage::createFromUserString('Invalid response');\n</code></pre></p> <p>\u2705 Specific feedback <pre><code>&lt;?php\n// GOOD: Clear, actionable feedback\nreturn LLMMessage::createFromUserString('The JSON is missing the required \"email\" field');\n</code></pre></p>"},{"location":"guides/feedback-loops/#see-also","title":"See Also","text":"<ul> <li>Tools Guide - Validate tool usage in feedback loops</li> <li>Reasoning Models - Combine reasoning with validation</li> <li>Examples - More feedback loop examples</li> <li>Configuration - Configure request behavior</li> </ul>"},{"location":"guides/multimodal/","title":"Multimodal Support","text":"<p>PHP LLM supports multimodal AI agents that can process both text and other content types like images and PDFs alongside your prompts.</p>"},{"location":"guides/multimodal/#sending-images","title":"Sending Images","text":"<p>AI agents can analyze images using the <code>LLMMessageImage</code> class. Images must be base64-encoded.</p>"},{"location":"guides/multimodal/#from-file-path","title":"From File Path","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\nuse Soukicz\\Llm\\Message\\LLMMessageImage;\nuse Soukicz\\Llm\\Message\\LLMMessageText;\n\n// Load and encode the image\n$imageData = base64_encode(file_get_contents('/path/to/image.jpg'));\n\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('What do you see in this image?'),\n    new LLMMessageImage('base64', 'image/jpeg', $imageData)\n]));\n</code></pre>"},{"location":"guides/multimodal/#different-image-types","title":"Different Image Types","text":"<pre><code>&lt;?php\n// JPEG image\n$jpegData = base64_encode(file_get_contents('/path/to/photo.jpg'));\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('Describe this image'),\n    new LLMMessageImage('base64', 'image/jpeg', $jpegData)\n]));\n\n// PNG image\n$pngData = base64_encode(file_get_contents('/path/to/diagram.png'));\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('Analyze this diagram'),\n    new LLMMessageImage('base64', 'image/png', $pngData)\n]));\n\n// WebP image\n$webpData = base64_encode(file_get_contents('/path/to/image.webp'));\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('What is in this image?'),\n    new LLMMessageImage('base64', 'image/webp', $webpData)\n]));\n</code></pre>"},{"location":"guides/multimodal/#sending-pdfs","title":"Sending PDFs","text":"<p>AI agents can read and analyze PDF documents using the <code>LLMMessagePdf</code> class. PDFs must be base64-encoded.</p>"},{"location":"guides/multimodal/#from-file-path_1","title":"From File Path","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\nuse Soukicz\\Llm\\Message\\LLMMessagePdf;\nuse Soukicz\\Llm\\Message\\LLMMessageText;\n\n// Load and encode the PDF\n$pdfData = base64_encode(file_get_contents('/path/to/document.pdf'));\n\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('Summarize this PDF document'),\n    new LLMMessagePdf('base64', $pdfData)\n]));\n</code></pre>"},{"location":"guides/multimodal/#extract-specific-information","title":"Extract Specific Information","text":"<pre><code>&lt;?php\n$pdfData = base64_encode(file_get_contents('/path/to/invoice.pdf'));\n\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('Extract the invoice number, date, and total amount from this PDF'),\n    new LLMMessagePdf('base64', $pdfData)\n]));\n</code></pre>"},{"location":"guides/multimodal/#complete-example","title":"Complete Example","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\nuse Soukicz\\Llm\\Client\\LLMChainClient;\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\nuse Soukicz\\Llm\\Message\\LLMMessageImage;\nuse Soukicz\\Llm\\Message\\LLMMessageText;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\n\n$cache = new FileCache(sys_get_temp_dir());\n$anthropic = new AnthropicClient('sk-xxxxx', $cache);\n$chainClient = new LLMChainClient();\n\n// Load and encode the image\n$imageData = base64_encode(file_get_contents('/path/to/photo.jpg'));\n\n$response = $chainClient-&gt;run(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUser(new LLMMessageContents([\n                new LLMMessageText('What objects are in this image?'),\n                new LLMMessageImage('base64', 'image/jpeg', $imageData)\n            ]))\n        ]),\n    )\n);\n\necho $response-&gt;getLastText();\n</code></pre>"},{"location":"guides/multimodal/#combining-multiple-media","title":"Combining Multiple Media","text":"<p>You can include multiple images and/or PDFs in a single message:</p> <pre><code>&lt;?php\n$image1Data = base64_encode(file_get_contents('/path/to/chart.png'));\n$image2Data = base64_encode(file_get_contents('/path/to/graph.jpg'));\n$pdfData = base64_encode(file_get_contents('/path/to/report.pdf'));\n\n$message = LLMMessage::createFromUser(new LLMMessageContents([\n    new LLMMessageText('Compare the data in these images with the PDF report'),\n    new LLMMessageImage('base64', 'image/png', $image1Data),\n    new LLMMessageImage('base64', 'image/jpeg', $image2Data),\n    new LLMMessagePdf('base64', $pdfData)\n]));\n</code></pre>"},{"location":"guides/multimodal/#provider-support","title":"Provider Support","text":"<p>Image Support: - \u2705 Anthropic (Claude) - All models - \u2705 OpenAI (GPT) - GPT-4o and later models - \u2705 Google Gemini - All 2.0+ models - \u26a0\ufe0f OpenAI-compatible - Depends on the underlying model</p> <p>PDF Support: - \u2705 Anthropic (Claude) - All models - \u2705 OpenAI (GPT) - GPT-4o and later models - \u274c Google Gemini - Not currently supported - \u26a0\ufe0f OpenAI-compatible - Depends on the underlying model</p>"},{"location":"guides/multimodal/#provider-specific-notes","title":"Provider-Specific Notes","text":""},{"location":"guides/multimodal/#openai-image-requirements","title":"OpenAI Image Requirements","text":"<p>OpenAI requires images to be encoded as base64 data URIs. The library handles the data URI formatting automatically.</p>"},{"location":"guides/multimodal/#anthropic-pdf-support","title":"Anthropic PDF Support","text":"<p>Anthropic's Claude models have excellent PDF parsing capabilities and can handle complex document layouts, tables, and multi-column text.</p>"},{"location":"guides/multimodal/#file-size-limits","title":"File Size Limits","text":"<p>Be aware of file size limits: - Images are typically limited to 5-20MB depending on provider - PDFs may have similar size restrictions - Large files will consume more tokens and increase costs</p>"},{"location":"guides/multimodal/#see-also","title":"See Also","text":"<ul> <li>Providers Documentation - Provider-specific multimodal features</li> <li>Tools Guide - Building agents that use tools with multimodal inputs</li> <li>Examples - More multimodal examples</li> </ul>"},{"location":"guides/reasoning/","title":"Reasoning Models","text":"<p>Reasoning models like OpenAI's o3 and o4 series spend additional computation time thinking through problems before responding. This makes them particularly effective for complex tasks requiring deep analysis, mathematics, coding, and logical reasoning.</p>"},{"location":"guides/reasoning/#overview","title":"Overview","text":"<p>Traditional language models generate responses token-by-token immediately. Reasoning models add an internal \"thinking\" phase where they: - Break down complex problems - Consider multiple approaches - Verify their reasoning - Refine their answers</p> <p>This results in more accurate responses for challenging tasks, at the cost of higher latency and token usage.</p>"},{"location":"guides/reasoning/#configuring-reasoning","title":"Configuring Reasoning","text":"<p>PHP LLM provides two ways to configure reasoning models:</p>"},{"location":"guides/reasoning/#reasoning-effort","title":"Reasoning Effort","text":"<p>Control how much computational effort the model spends reasoning:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPTo3;\nuse Soukicz\\Llm\\Config\\ReasoningEffort;\nuse Soukicz\\Llm\\LLMRequest;\n\n$request = new LLMRequest(\n    model: new GPTo3(GPTo3::VERSION_2025_04_16),\n    conversation: $conversation,\n    reasoningConfig: ReasoningEffort::HIGH\n);\n</code></pre> <p>Effort Levels: - <code>ReasoningEffort::LOW</code> - Fast, less thorough reasoning - <code>ReasoningEffort::MEDIUM</code> - Balanced reasoning (default) - <code>ReasoningEffort::HIGH</code> - Thorough, slower reasoning</p>"},{"location":"guides/reasoning/#reasoning-budget","title":"Reasoning Budget","text":"<p>Set a token limit for the model's internal reasoning:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Config\\ReasoningBudget;\n\n$request = new LLMRequest(\n    model: new GPTo3(GPTo3::VERSION_2025_04_16),\n    conversation: $conversation,\n    reasoningConfig: new ReasoningBudget(10000) // Max 10k tokens for reasoning\n);\n</code></pre>"},{"location":"guides/reasoning/#complete-example","title":"Complete Example","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAIClient;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPTo3;\nuse Soukicz\\Llm\\Client\\LLMChainClient;\nuse Soukicz\\Llm\\Config\\ReasoningEffort;\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\n\n$cache = new FileCache(sys_get_temp_dir());\n$openai = new OpenAIClient('sk-xxxxx', 'org-xxxxx', $cache);\n$chainClient = new LLMChainClient();\n\n$response = $chainClient-&gt;run(\n    client: $openai,\n    request: new LLMRequest(\n        model: new GPTo3(GPTo3::VERSION_2025_04_16),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString(\n                'A farmer has 17 sheep. All but 9 die. How many sheep are left alive?'\n            )\n        ]),\n        reasoningEffort: ReasoningEffort::HIGH\n    )\n);\n\necho $response-&gt;getLastText(); // \"9 sheep are left alive\"\n</code></pre>"},{"location":"guides/reasoning/#when-to-use-reasoning-models","title":"When to Use Reasoning Models","text":"<p>Ideal Use Cases: - \u2705 Complex mathematical problems - \u2705 Advanced coding challenges - \u2705 Logical puzzles and riddles - \u2705 Scientific analysis - \u2705 Multi-step problem solving - \u2705 Tasks requiring verification</p> <p>Not Ideal For: - \u274c Simple queries - \u274c Creative writing - \u274c Casual conversation - \u274c Tasks requiring fast responses - \u274c Cost-sensitive applications</p>"},{"location":"guides/reasoning/#supported-models","title":"Supported Models","text":""},{"location":"guides/reasoning/#openai-reasoning-models","title":"OpenAI Reasoning Models","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPTo3;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPTo4Mini;\n\n// o3 - Most capable reasoning model\n$o3 = new GPTo3(GPTo3::VERSION_2025_04_16);\n\n// o4-mini - Faster, more cost-effective reasoning\n$o4mini = new GPTo4Mini(GPTo4Mini::VERSION_2025_04_16);\n</code></pre>"},{"location":"guides/reasoning/#cost-considerations","title":"Cost Considerations","text":"<p>Reasoning models consume significantly more tokens due to their internal thinking process:</p> <ol> <li>Input tokens - Your prompt (standard pricing)</li> <li>Reasoning tokens - Internal thinking (usually discounted pricing)</li> <li>Output tokens - The response (standard pricing)</li> </ol> <p>Use <code>ReasoningBudget</code> to control costs:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Config\\ReasoningBudget;\n\n// Limit reasoning to 5000 tokens for cost control\n$request = new LLMRequest(\n    model: new GPTo3(GPTo3::VERSION_2025_04_16),\n    conversation: $conversation,\n    reasoningConfig: new ReasoningBudget(5000)\n);\n</code></pre>"},{"location":"guides/reasoning/#tracking-reasoning-usage","title":"Tracking Reasoning Usage","text":"<p>Monitor token usage including reasoning tokens:</p> <pre><code>&lt;?php\n$response = $chainClient-&gt;run($client, $request);\n$usage = $response-&gt;getTokenUsage();\n\necho \"Input tokens: \" . $usage-&gt;getInputTokens() . \"\\n\";\necho \"Reasoning tokens: \" . $usage-&gt;getReasoningTokens() . \"\\n\";\necho \"Output tokens: \" . $usage-&gt;getOutputTokens() . \"\\n\";\necho \"Total cost: $\" . $usage-&gt;getTotalCost() . \"\\n\";\n</code></pre>"},{"location":"guides/reasoning/#combining-with-other-features","title":"Combining with Other Features","text":""},{"location":"guides/reasoning/#with-tools","title":"With Tools","text":"<p>Reasoning models work excellently with tools for complex agent workflows:</p> <pre><code>&lt;?php\n$request = new LLMRequest(\n    model: new GPTo3(GPTo3::VERSION_2025_04_16),\n    conversation: $conversation,\n    tools: [$calculatorTool, $databaseTool],\n    reasoningConfig: ReasoningEffort::HIGH\n);\n</code></pre>"},{"location":"guides/reasoning/#with-feedback-loops","title":"With Feedback Loops","text":"<p>Combine reasoning with validation for ultra-reliable agents:</p> <pre><code>&lt;?php\n$response = $chainClient-&gt;run(\n    client: $openai,\n    request: new LLMRequest(\n        model: new GPTo3(GPTo3::VERSION_2025_04_16),\n        conversation: $conversation,\n        reasoningConfig: ReasoningEffort::HIGH\n    ),\n    feedbackCallback: function ($response) {\n        // Validate the reasoning model's output\n        return $isValid ? null : LLMMessage::createFromUserString('Please reconsider...');\n    }\n);\n</code></pre>"},{"location":"guides/reasoning/#best-practices","title":"Best Practices","text":"<ol> <li>Start with MEDIUM effort - Only increase if needed</li> <li>Set budgets for production - Prevent runaway costs</li> <li>Use for appropriate tasks - Don't use reasoning models for simple queries</li> <li>Monitor costs closely - Track token usage and adjust budgets</li> <li>Test with o4-mini first - More cost-effective for development</li> </ol>"},{"location":"guides/reasoning/#provider-support","title":"Provider Support","text":"<ul> <li>\u2705 OpenAI - o3, o4-mini (native reasoning support)</li> <li>\u274c Anthropic - Not available (Claude uses different architecture)</li> <li>\u274c Google Gemini - Not available</li> <li>\u26a0\ufe0f OpenAI-compatible - Depends on provider</li> </ul>"},{"location":"guides/reasoning/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - All request configuration options</li> <li>Feedback Loops - Validate reasoning outputs</li> <li>OpenAI Provider Documentation - OpenAI-specific features</li> </ul>"},{"location":"guides/tools/","title":"Tools &amp; Function Calling","text":"<p>Tools (also known as function calling) empower your AI agents to interact with external systems, APIs, and data sources. This is a core capability for building agentic systems that can take actions beyond generating text.</p>"},{"location":"guides/tools/#overview","title":"Overview","text":"<p>When you provide tools to an AI agent, the model can: 1. Decide when a tool is needed to answer a query 2. Select the appropriate tool 3. Generate the correct input parameters 4. Receive the tool's output 5. Incorporate the results into its response</p>"},{"location":"guides/tools/#defining-tools","title":"Defining Tools","text":"<p>Use <code>CallbackToolDefinition</code> to define tools with callback functions:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Tool\\CallbackToolDefinition;\n\n$tool = new CallbackToolDefinition(\n    name: 'tool_name',\n    description: 'Clear description of what the tool does and when to use it',\n    inputSchema: [\n        'type' =&gt; 'object',\n        'properties' =&gt; [\n            'param1' =&gt; ['type' =&gt; 'string', 'description' =&gt; 'Parameter description'],\n            'param2' =&gt; ['type' =&gt; 'number', 'description' =&gt; 'Another parameter'],\n        ],\n        'required' =&gt; ['param1'],\n    ],\n    handler: function (array $input) {\n        // Your tool logic here\n        return $result;\n    }\n);\n</code></pre>"},{"location":"guides/tools/#complete-example-currency-converter-agent","title":"Complete Example: Currency Converter Agent","text":"<pre><code>&lt;?php\nuse GuzzleHttp\\Client;\nuse GuzzleHttp\\Promise\\PromiseInterface;\nuse GuzzleHttp\\Psr7\\Response;\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\nuse Soukicz\\Llm\\Client\\LLMChainClient;\nuse Soukicz\\Llm\\Message\\LLMMessage;\nuse Soukicz\\Llm\\Message\\LLMMessageContents;\nuse Soukicz\\Llm\\LLMConversation;\nuse Soukicz\\Llm\\LLMRequest;\nuse Soukicz\\Llm\\Tool\\CallbackToolDefinition;\n\nrequire_once __DIR__ . '/vendor/autoload.php';\n\n$cache = new FileCache(sys_get_temp_dir());\n$anthropic = new AnthropicClient('sk-xxxxxx', $cache);\n$chainClient = new LLMChainClient();\n\n$currencyTool = new CallbackToolDefinition(\n    name: 'currency_rates',\n    description: 'Tool for getting current currency rates. Required input is currency code of source currency and currency code of target currency.',\n    inputSchema: [\n        'type' =&gt; 'object',\n        'properties' =&gt; [\n            'source_currency' =&gt; ['type' =&gt; 'string'],\n            'target_currency' =&gt; ['type' =&gt; 'string'],\n        ],\n        'required' =&gt; ['source_currency', 'target_currency'],\n    ],\n    handler: function (array $input): PromiseInterface {\n        $client = new Client();\n\n        // Tool can return either a promise or a value\n        return $client-&gt;getAsync('https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api@latest/v1/currencies/' . strtolower($input['source_currency']) . '.json')\n            -&gt;then(function (Response $response) use ($input) {\n                $data = json_decode($response-&gt;getBody()-&gt;getContents(), true, 512, JSON_THROW_ON_ERROR);\n\n                return LLMMessageContents::fromArrayData([\n                    'rate' =&gt; $data[strtolower($input['source_currency'])][strtolower($input['target_currency'])],\n                ]);\n            });\n    }\n);\n\n$response = $chainClient-&gt;run(\n    client: $anthropic,\n    request: new LLMRequest(\n        model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n        conversation: new LLMConversation([\n            LLMMessage::createFromUserString('How much is 100 USD in EUR today?')\n        ]),\n        tools: [$currencyTool],\n    )\n);\n\necho $response-&gt;getLastText();\n</code></pre>"},{"location":"guides/tools/#tool-handler-return-types","title":"Tool Handler Return Types","text":"<p>Tool handlers must return either:</p> <ol> <li>LLMMessageContents - Structured response data (recommended)</li> <li>Promises - For async operations (recommended for API calls)</li> </ol> <pre><code>&lt;?php\n// LLMMessageContents (recommended for synchronous operations)\nhandler: function (array $input): LLMMessageContents {\n    return LLMMessageContents::fromArrayData(['key' =&gt; 'value']);\n}\n\n// Promise (recommended for async operations like API calls)\nhandler: function (array $input): PromiseInterface {\n    return $httpClient-&gt;getAsync($url)\n        -&gt;then(function ($response) {\n            $data = json_decode($response-&gt;getBody(), true);\n            return LLMMessageContents::fromArrayData($data);\n        });\n}\n</code></pre> <p>Note: Tool handlers cannot return plain arrays or scalar values. Always wrap your results in <code>LLMMessageContents::fromArrayData()</code>.</p> <p>Tip: If you need to convert <code>LLMMessageContents</code> back to a plain array (e.g., for testing), use the <code>toArray()</code> method: <pre><code>&lt;?php\n$result = $tool-&gt;handle(['input' =&gt; 'value']);\n$array = $result-&gt;toArray();  // Converts to plain array\n</code></pre></p>"},{"location":"guides/tools/#built-in-tools","title":"Built-in Tools","text":""},{"location":"guides/tools/#text-editor-tool","title":"Text Editor Tool","text":"<p>For building file-manipulation agents with Anthropic models, you can use the <code>TextEditorTool</code>. This tool requires a custom storage implementation.</p> <p>Note: This is an advanced feature that requires implementing the <code>TextEditorStorage</code> interface to handle file operations securely.</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Tool\\TextEditor\\TextEditorTool;\nuse Soukicz\\Llm\\Tool\\TextEditor\\TextEditorStorage;\n\n// You need to implement TextEditorStorage interface\nclass MyTextEditorStorage implements TextEditorStorage {\n    public function __construct(private string $basePath) {}\n\n    public function read(string $path): string {\n        // Implement secure file reading\n        return file_get_contents($this-&gt;basePath . '/' . $path);\n    }\n\n    public function write(string $path, string $content): void {\n        // Implement secure file writing\n        file_put_contents($this-&gt;basePath . '/' . $path, $content);\n    }\n\n    // Implement other required methods...\n}\n\n$storage = new MyTextEditorStorage('/path/to/working/directory');\n$textEditorTool = new TextEditorTool($storage);\n\n$request = new LLMRequest(\n    model: new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929),\n    conversation: new LLMConversation([\n        LLMMessage::createFromUserString('Create a new file called hello.txt with \"Hello World\"')\n    ]),\n    tools: [$textEditorTool],\n);\n</code></pre> <p>The text editor tool supports: - <code>view</code> - View file contents - <code>create</code> - Create new files - <code>str_replace</code> - Replace text in files - <code>insert</code> - Insert text at specific line numbers</p> <p>Security Considerations: When implementing <code>TextEditorStorage</code>, ensure proper path validation, access controls, and sandboxing to prevent unauthorized file access.</p>"},{"location":"guides/tools/#input-schema","title":"Input Schema","text":"<p>The <code>inputSchema</code> follows JSON Schema specification. Common patterns:</p>"},{"location":"guides/tools/#simple-parameters","title":"Simple Parameters","text":"<pre><code>&lt;?php\n'inputSchema' =&gt; [\n    'type' =&gt; 'object',\n    'properties' =&gt; [\n        'query' =&gt; ['type' =&gt; 'string', 'description' =&gt; 'Search query'],\n        'limit' =&gt; ['type' =&gt; 'integer', 'description' =&gt; 'Max results'],\n    ],\n    'required' =&gt; ['query'],\n]\n</code></pre>"},{"location":"guides/tools/#enums","title":"Enums","text":"<pre><code>&lt;?php\n'properties' =&gt; [\n    'priority' =&gt; [\n        'type' =&gt; 'string',\n        'enum' =&gt; ['low', 'medium', 'high'],\n        'description' =&gt; 'Task priority level'\n    ],\n]\n</code></pre>"},{"location":"guides/tools/#arrays","title":"Arrays","text":"<pre><code>&lt;?php\n'properties' =&gt; [\n    'tags' =&gt; [\n        'type' =&gt; 'array',\n        'items' =&gt; ['type' =&gt; 'string'],\n        'description' =&gt; 'List of tags'\n    ],\n]\n</code></pre>"},{"location":"guides/tools/#nested-objects","title":"Nested Objects","text":"<pre><code>&lt;?php\n'properties' =&gt; [\n    'location' =&gt; [\n        'type' =&gt; 'object',\n        'properties' =&gt; [\n            'lat' =&gt; ['type' =&gt; 'number'],\n            'lng' =&gt; ['type' =&gt; 'number'],\n        ],\n        'required' =&gt; ['lat', 'lng'],\n    ],\n]\n</code></pre>"},{"location":"guides/tools/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Descriptions - Provide detailed descriptions for both the tool and each parameter</li> <li>Use Promises for I/O - Return promises for async operations like API calls</li> <li>Validate Input - The schema helps, but add runtime validation if needed</li> <li>Error Handling - Handle errors gracefully and return meaningful error messages</li> <li>Keep Tools Focused - One tool should do one thing well</li> <li>Document Side Effects - If a tool modifies state, make it clear in the description</li> </ol>"},{"location":"guides/tools/#provider-support","title":"Provider Support","text":"<ul> <li>\u2705 Anthropic (Claude) - Full support, including native tools</li> <li>\u2705 OpenAI (GPT) - Full function calling support</li> <li>\u2705 Google Gemini - Full function calling support</li> <li>\u26a0\ufe0f OpenAI-compatible - Depends on the underlying model</li> </ul>"},{"location":"guides/tools/#see-also","title":"See Also","text":"<ul> <li>Feedback Loops - Validate tool outputs</li> <li>Examples - More tool examples</li> <li>Provider Documentation - Provider-specific tool features</li> </ul>"},{"location":"providers/","title":"Provider Usage Guide","text":"<p>This guide shows how to use each provider client in PHP LLM. All providers share the same <code>LLMRequest</code> API but have different client instantiation and configuration options.</p>"},{"location":"providers/#client-interface-support","title":"Client Interface Support","text":"Client Implements LLMClient Implements LLMBatchClient Constructor Parameters <code>AnthropicClient</code> \u2705 \u2705 <code>apiKey</code>, <code>cache</code>, <code>handler</code> <code>OpenAIClient</code> \u2705 \u2705 <code>apiKey</code>, <code>apiOrganization</code>, <code>cache</code>, <code>handler</code> <code>GeminiClient</code> \u2705 \u274c <code>apiKey</code>, <code>cache</code>, <code>safetySettings</code>, <code>handler</code> <code>OpenAICompatibleClient</code> \u2705 Varies <code>apiKey</code>, <code>baseUrl</code>, <code>cache</code>, <code>handler</code>"},{"location":"providers/#anthropic-claude","title":"Anthropic (Claude)","text":""},{"location":"providers/#client-instantiation","title":"Client Instantiation","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\n\n$cache = new FileCache(sys_get_temp_dir());\n$client = new AnthropicClient(\n    apiKey: 'sk-ant-xxxxx',\n    cache: $cache,           // Optional: CacheInterface\n    handler: $handlerStack   // Optional: Guzzle HandlerStack for middleware\n);\n</code></pre>"},{"location":"providers/#model-classes","title":"Model Classes","text":"<p>All Anthropic models require a version constant:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude45Sonnet;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude35Haiku;\nuse Soukicz\\Llm\\Client\\Anthropic\\Model\\AnthropicClaude37Sonnet;\n\n// Must specify version when instantiating\n$model = new AnthropicClaude45Sonnet(AnthropicClaude45Sonnet::VERSION_20250929);\n$model = new AnthropicClaude35Haiku(AnthropicClaude35Haiku::VERSION_20241022);\n</code></pre> <p>Available model classes: - <code>AnthropicClaude35Sonnet</code>, <code>AnthropicClaude35Haiku</code> - <code>AnthropicClaude37Sonnet</code> - <code>AnthropicClaude4Sonnet</code>, <code>AnthropicClaude4Opus</code> - <code>AnthropicClaude41Opus</code> - <code>AnthropicClaude45Sonnet</code></p>"},{"location":"providers/#batch-processing-support","title":"Batch Processing Support","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\LLMBatchClient;\n\n// AnthropicClient implements LLMBatchClient\n/** @var LLMBatchClient $client */\n$batchId = $client-&gt;createBatch($requests);\n$batch = $client-&gt;retrieveBatch($batchId);\n</code></pre>"},{"location":"providers/#openai-gpt","title":"OpenAI (GPT)","text":""},{"location":"providers/#client-instantiation_1","title":"Client Instantiation","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAIClient;\n\n$cache = new FileCache(sys_get_temp_dir());\n$client = new OpenAIClient(\n    apiKey: 'sk-xxxxx',\n    apiOrganization: 'org-xxxxx',  // Optional: for organization accounts\n    cache: $cache,                  // Optional: CacheInterface\n    handler: $handlerStack          // Optional: Guzzle HandlerStack\n);\n</code></pre>"},{"location":"providers/#model-classes_1","title":"Model Classes","text":"<p>All OpenAI models require a version constant:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPT5;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPT4o;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPTo3;\nuse Soukicz\\Llm\\Client\\OpenAI\\Model\\GPTo4Mini;\n\n// All models require version parameter\n$model = new GPT5(GPT5::VERSION_2025_08_07);\n$model = new GPT4o(GPT4o::VERSION_2024_11_20);\n$model = new GPTo3(GPTo3::VERSION_2025_04_16);\n$model = new GPTo4Mini(GPTo4Mini::VERSION_2025_04_16);\n</code></pre> <p>Available model classes: - <code>GPT4o</code>, <code>GPT4oMini</code> - <code>GPT41</code>, <code>GPT41Mini</code>, <code>GPT41Nano</code> - <code>GPTo3</code>, <code>GPTo4Mini</code> (reasoning models) - <code>GPT5</code>, <code>GPT5Mini</code>, <code>GPT5Nano</code></p> <p>Note: Each model class has version constants defined (e.g., <code>GPT5::VERSION_2025_08_07</code>). Check the class for available versions.</p>"},{"location":"providers/#reasoning-model-configuration","title":"Reasoning Model Configuration","text":"<p>Use <code>reasoningEffort</code> or <code>reasoningConfig</code> in <code>LLMRequest</code>:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Config\\ReasoningEffort;\nuse Soukicz\\Llm\\Config\\ReasoningBudget;\n\n$request = new LLMRequest(\n    model: new GPTo3(GPTo3::VERSION_2025_04_16),\n    conversation: $conversation,\n    reasoningEffort: ReasoningEffort::HIGH  // or ReasoningEffort::LOW, ::MEDIUM\n);\n\n// Or use token budget\n$request = new LLMRequest(\n    model: new GPTo3(GPTo3::VERSION_2025_04_16),\n    conversation: $conversation,\n    reasoningConfig: new ReasoningBudget(10000)\n);\n</code></pre>"},{"location":"providers/#batch-processing-support_1","title":"Batch Processing Support","text":"<pre><code>&lt;?php\n// OpenAIClient implements LLMBatchClient\n$batchId = $client-&gt;createBatch($requests);\n$batch = $client-&gt;retrieveBatch($batchId);\n</code></pre>"},{"location":"providers/#google-gemini","title":"Google Gemini","text":""},{"location":"providers/#client-instantiation_2","title":"Client Instantiation","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\Gemini\\GeminiClient;\n\n$cache = new FileCache(sys_get_temp_dir());\n$client = new GeminiClient(\n    apiKey: 'your-api-key',\n    cache: $cache,              // Optional: CacheInterface\n    safetySettings: [],         // Optional: array of safety settings (see below)\n    handler: $handlerStack      // Optional: Guzzle HandlerStack\n);\n</code></pre>"},{"location":"providers/#model-classes_2","title":"Model Classes","text":"<pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\Gemini\\Model\\Gemini25Pro;\nuse Soukicz\\Llm\\Client\\Gemini\\Model\\Gemini25Flash;\nuse Soukicz\\Llm\\Client\\Gemini\\Model\\Gemini20Flash;\n\n// Models don't require version parameters\n$model = new Gemini25Pro();\n$model = new Gemini25Flash();\n</code></pre> <p>Available model classes: - <code>Gemini20Flash</code>, <code>Gemini20FlashLite</code> - <code>Gemini25Flash</code>, <code>Gemini25FlashLite</code> - <code>Gemini25FlashImagePreview</code> - <code>Gemini25Pro</code>, <code>Gemini25ProPreview</code></p>"},{"location":"providers/#safety-settings-configuration","title":"Safety Settings Configuration","text":"<p>GeminiClient accepts safety settings as an array in the constructor:</p> <pre><code>&lt;?php\n// Permissive settings (minimal blocking)\n$permissiveSettings = [\n    ['category' =&gt; 'HARM_CATEGORY_HARASSMENT', 'threshold' =&gt; 'BLOCK_NONE'],\n    ['category' =&gt; 'HARM_CATEGORY_HATE_SPEECH', 'threshold' =&gt; 'BLOCK_NONE'],\n    ['category' =&gt; 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold' =&gt; 'BLOCK_NONE'],\n    ['category' =&gt; 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold' =&gt; 'BLOCK_NONE'],\n];\n\n// Strict settings (block more content)\n$strictSettings = [\n    ['category' =&gt; 'HARM_CATEGORY_HARASSMENT', 'threshold' =&gt; 'BLOCK_LOW_AND_ABOVE'],\n    ['category' =&gt; 'HARM_CATEGORY_HATE_SPEECH', 'threshold' =&gt; 'BLOCK_LOW_AND_ABOVE'],\n    ['category' =&gt; 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold' =&gt; 'BLOCK_LOW_AND_ABOVE'],\n    ['category' =&gt; 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold' =&gt; 'BLOCK_LOW_AND_ABOVE'],\n];\n\n$client = new GeminiClient(\n    apiKey: 'your-api-key',\n    safetySettings: $permissiveSettings\n);\n</code></pre> <p>Available thresholds: - <code>BLOCK_NONE</code> - No blocking - <code>BLOCK_ONLY_HIGH</code> - Block high severity only - <code>BLOCK_MEDIUM_AND_ABOVE</code> - Block medium and high - <code>BLOCK_LOW_AND_ABOVE</code> - Block low, medium, and high</p>"},{"location":"providers/#batch-processing","title":"Batch Processing","text":"<p>\u26a0\ufe0f GeminiClient does not implement <code>LLMBatchClient</code>. Batch processing is not supported for Gemini through this library.</p>"},{"location":"providers/#limitations","title":"Limitations","text":"<ul> <li>No PDF support - Gemini models don't accept PDF inputs through this library</li> <li>No batch processing - Must process requests individually</li> </ul>"},{"location":"providers/#openai-compatible-providers","title":"OpenAI-Compatible Providers","text":""},{"location":"providers/#client-instantiation_3","title":"Client Instantiation","text":"<p>Use <code>OpenAICompatibleClient</code> for any service that implements the OpenAI API specification:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAICompatibleClient;\n\n$cache = new FileCache(sys_get_temp_dir());\n$client = new OpenAICompatibleClient(\n    apiKey: 'your-api-key',\n    baseUrl: 'https://api.provider.com/v1',  // Required: API endpoint\n    cache: $cache,                            // Optional: CacheInterface\n    handler: $handlerStack                    // Optional: Guzzle HandlerStack\n);\n</code></pre>"},{"location":"providers/#model-usage","title":"Model Usage","text":"<p>Use <code>LocalModel</code> to specify any model name as a string:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\Universal\\LocalModel;\n\n// Model name as provided by the API service\n$model = new LocalModel('model-name');\n\n// The model name is passed directly to the API\n$request = new LLMRequest(\n    model: new LocalModel('llama-3.2-8b'),\n    conversation: $conversation\n);\n</code></pre>"},{"location":"providers/#provider-specific-examples","title":"Provider-Specific Examples","text":""},{"location":"providers/#openrouter","title":"OpenRouter","text":"<pre><code>&lt;?php\n$client = new OpenAICompatibleClient(\n    apiKey: 'sk-or-v1-xxxxx',\n    baseUrl: 'https://openrouter.ai/api/v1'\n);\n\n// Use OpenRouter's model naming format\n$model = new LocalModel('anthropic/claude-3.5-sonnet');\n$model = new LocalModel('openai/gpt-4o');\n$model = new LocalModel('meta-llama/llama-3.2-8b-instruct');\n</code></pre>"},{"location":"providers/#ollama-local","title":"Ollama (Local)","text":"<pre><code>&lt;?php\n$client = new OpenAICompatibleClient(\n    apiKey: 'not-needed',  // Ollama doesn't require API key\n    baseUrl: 'http://localhost:11434/v1'\n);\n\n// Use Ollama model names\n$model = new LocalModel('llama3.2');\n$model = new LocalModel('mistral');\n</code></pre>"},{"location":"providers/#llama-server-local","title":"llama-server (Local)","text":"<pre><code>&lt;?php\n$client = new OpenAICompatibleClient(\n    apiKey: 'not-needed',\n    baseUrl: 'http://localhost:8080/v1'\n);\n\n$model = new LocalModel('local-model');\n</code></pre>"},{"location":"providers/#feature-support","title":"Feature Support","text":"<p>Feature availability depends entirely on the underlying provider/model: - Function calling: Check if provider supports tools - Multimodal: Check if model accepts images/PDFs - Batch processing: Most compatible APIs don't support batching - Reasoning: Only if provider offers reasoning models</p>"},{"location":"providers/#aws-bedrock","title":"AWS Bedrock","text":"<p>AWS Bedrock support is available through a separate extension package:</p> <pre><code>composer require soukicz/llm-aws-bedrock\n</code></pre> <p>This package provides a <code>BedrockClient</code> that implements the same <code>LLMClient</code> interface. See the extension documentation for setup and usage details.</p>"},{"location":"providers/#using-multiple-providers","title":"Using Multiple Providers","text":"<p>You can use multiple providers in the same application by instantiating different clients:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Client\\Anthropic\\AnthropicClient;\nuse Soukicz\\Llm\\Client\\OpenAI\\OpenAIClient;\nuse Soukicz\\Llm\\Client\\Gemini\\GeminiClient;\n\n// Instantiate all providers\n$anthropic = new AnthropicClient('sk-ant-xxxxx', $cache);\n$openai = new OpenAIClient('sk-xxxxx', 'org-xxxxx', $cache);\n$gemini = new GeminiClient('gemini-key', $cache);\n\n// Use the same LLMChainClient with any provider\n$chainClient = new LLMChainClient();\n\n// Switch between providers by changing the client\n$response = $chainClient-&gt;run($anthropic, $request);\n$response = $chainClient-&gt;run($openai, $request);\n$response = $chainClient-&gt;run($gemini, $request);\n</code></pre>"},{"location":"providers/#common-configuration","title":"Common Configuration","text":""},{"location":"providers/#caching","title":"Caching","text":"<p>All clients support HTTP-level caching through the <code>CacheInterface</code>:</p> <pre><code>&lt;?php\nuse Soukicz\\Llm\\Cache\\FileCache;\n\n$cache = new FileCache('/path/to/cache/dir');\n\n// Pass the same cache instance to all clients\n$anthropic = new AnthropicClient('key', $cache);\n$openai = new OpenAIClient('key', null, $cache);\n$gemini = new GeminiClient('key', $cache);\n</code></pre>"},{"location":"providers/#guzzle-middleware","title":"Guzzle Middleware","text":"<p>All clients accept a Guzzle <code>HandlerStack</code> for custom middleware (logging, retries, etc.):</p> <pre><code>&lt;?php\nuse GuzzleHttp\\HandlerStack;\nuse GuzzleHttp\\Middleware;\n\n$stack = HandlerStack::create();\n$stack-&gt;push(Middleware::log($logger, $messageFormatter));\n\n$client = new AnthropicClient(\n    apiKey: 'key',\n    cache: $cache,\n    handler: $stack\n);\n</code></pre>"},{"location":"providers/#environment-variables","title":"Environment Variables","text":"<p>Store API keys in environment variables:</p> <pre><code>&lt;?php\n$anthropic = new AnthropicClient(\n    apiKey: getenv('ANTHROPIC_API_KEY'),\n    cache: $cache\n);\n\n$openai = new OpenAIClient(\n    apiKey: getenv('OPENAI_API_KEY'),\n    apiOrganization: getenv('OPENAI_ORG_ID'),\n    cache: $cache\n);\n</code></pre>"},{"location":"providers/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - Request configuration options</li> <li>Multimodal Guide - Using images and PDFs</li> <li>Reasoning Guide - OpenAI reasoning models</li> <li>Batch Processing Guide - Anthropic and OpenAI batch APIs</li> </ul>"}]}